[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling - 24DS636 (2024-25)",
    "section": "",
    "text": "Course Introduction\nThe website contains course contents of “Statistical Modeling” offered by Abhijith M S, PhD to Masters students pursuing M.Tech in Data Science, during the even semester of the academic year 2024-25.\n\n\nSyllabus\n(As given in the curriculum)\n\nProbability, Random Variables & Probability Distributions.\nSampling, analysis of sample data-Empirical Distributions, Sampling from a Population Estimation, confidence intervals, point estimation–Maximum Likelihood, Probability mass functions, Modeling distributions, Hypothesis testing- Z, t, Chi-Square.\nANOVA & Designs of Experiments - Single, Two factor ANOVA, Factorials ANOVA models.\nLinear least squares, Correlation & Regression Models-linear regression methods, Ridge regression, LASSO, univariate and Multivariate Linear Regression, probabilistic interpretation, Regularization, Logistic regression, locally weighted regression.\nExploratory data analysis, Time series analysis, Analytical methods – ARIMA and SARIMA.\n\n\n\nEvaluations: A Tentative Timeline\n\nBest two marks out of three quizzes (Total = 20 marks)\nQuiz-1 (10 marks): (January First week)\nQuiz-2 (10 marks):(March First week)\nQuiz-3 (10 marks):(April First week)\nAssignments (Total = 30 marks)\nAssignment-1 (10 marks):(Submission: End of January)\nAssignment-2 (10 marks):(Submission: End of March)\nProject Review - 1 (10 marks):(February second week)\nMid Sem (Total = 20 marks)\nMid-Semester Exam (20 marks):(Feb first week, as per Academic calender)\nEnd Sem (Total = 30 marks)\nEnd-Semester Project Presentation (20 marks):(April second week, as per Academic calender) \\end{itemize}$\n\nContact: ms_abhijith@cb.amrita.edu",
    "crumbs": [
      "Course Introduction"
    ]
  },
  {
    "objectID": "DescriptiveStatistics.html",
    "href": "DescriptiveStatistics.html",
    "title": "1  Descriptive statistics",
    "section": "",
    "text": "Descriptive statistics deals with methods to describe and summarize data.\nDescribing of data is effectively done through tables or graphs. Those often reveal important features such as the range, the degree of concentration, and the symmetry of the data.\nThe summary of data is expressed through numerical quantities (summary statistics) whose values are determined by the data.\n\n\n1.0.1 Describing Data sets\n\n\n1.0.2 Frequency Tables and Graphs\n\nA data set having a relatively small number of distinct values can be conveniently presented in a frequency table.\nData from a frequency table can be graphically represented by:\n\nLine Graph\nBar Graph\nFrequency Polygon\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\n# Line Graph\nplt.plot(values, frequencies, marker='o')\nplt.title('Frequency Polygon')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\nplt.bar(values, frequencies, width=0.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.3 Relative Frequency Tables and Graphs\n\nConsider a data set consisting of n values. If f is the frequency of a particular value, then the ratio f /n is called its relative frequency.\nThat is, the relative frequency of a data value is the proportion of the data that have that value.\nThe relative frequencies can be represented graphically by:\n\nrelative frequency line\nrelative frequency bar graph\nrelative frequency polygon\npie chart: A pie chart is often used to indicate relative frequencies when the data are not numerical in nature. A circle is constructed and then sliced into different sectors with areas proportional to the respective relative frequencies.\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\nrelative_frequencies = frequencies/len(data)\nprint(relative_frequencies)\n\n\nplt.pie(relative_frequencies, labels = values, autopct='%1.1f%%')\nplt.show()\n\n[0.2        0.46666667 0.33333333]\n\n\n\n\n\n\n\n\n\n\n\n1.0.4 Grouped Data, Histograms, Ogives, and Stem and Leaf Plots\n\nFor some data sets the number of distinct values is too large to utilize frequency tables.\nInstead, in such cases, it is useful to divide the values into groupings, or class intervals, and then plot the number of data values falling in each class interval.\nThe number of class intervals chosen should be a trade-off between:\n\nchoosing too few classes at a cost of losing too much information about the actual data values in a class.\nchoosing too many classes, which will result in the frequencies of each class being too small.\n\nIt is common, although not essential, to choose class intervals of equal length.\nThe endpoints of a class interval are called the class boundaries.\nWe will adopt the left-end inclusion convention, which stipulates that a class interval contains its left-end but not its right-end boundary point.\nThus, for instance, the class interval 20-30 contains all values that are both greater than or equal to 20 and less than 30.\nA bar graph plot of class data, with the bars placed adjacent to each other, is called a histogram.\nThe vertical axis of a histogram can represent either the class frequency or the relative class frequency.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\n# Plot histogram\nplt.hist(random_integers, bins=bins, edgecolor='black')\nplt.title('Histogram of Random Integers')\nplt.xlabel('Class Interval')\nplt.ylabel('Frequency')\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n\n\n\n\n\n\n\n\n\n\nWe are sometimes interested in plotting a cumulative frequency (or cumulative relative frequency) graph.\nA point on the horizontal axis of such a graph represents a possible data value; its corresponding vertical plot gives the number (or proportion) of the data whose values are less than or equal to it.\nA cumulative frequency plot is called an ogive.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\nhistograms = np.histogram(random_integers, bins=bins)[0]\nprint(histograms)\ncumulativeSum = np.cumsum(histograms) \nprint(cumulativeSum)\n\n\nplt.plot(bins[:-1], cumulativeSum, marker='o', linestyle='-')\nplt.title('Ogive (Cumulative Frequency Graph)')\nplt.xlabel('Class Interval')\nplt.ylabel('Cumulative Frequency')\n#plt.grid(True)\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n[22 25 15 24 14 21 20 24 19 21 23 21 28 23]\n[ 22  47  62  86 100 121 141 165 184 205 228 249 277 300]\n\n\n\n\n\n\n\n\n\n\nAn efficient way of organizing a small-to moderate-sized data set is to utilize a stem and leaf plot.\nSuch a plot is obtained by first dividing each data value into two parts - its stem and its leaf.\nFor instance, if the data are all two-digit numbers, then we could let the stem part of a data value be its tens digit and let the leaf be its ones digit.\nThus, for instance, the value 62 is expressed as\n\n\nimport numpy as np\n\n# Sample data\ndata = np.array([62, 67, 63, 68, 69, 61, 64, 65, 66, 60, 75, 74, 76, 78, 90, 92, 34, 36, 56, 57, 45, 53, 52, 59, 73, 74, 79, 80, 81,20, 34, 36, 85])\n\n# Create stem and leaf plot\nstem_leaf = {}\n\nfor number in data:\n    stem = number // 10\n    leaf = number % 10\n    if stem in stem_leaf:\n        stem_leaf[stem].append(leaf)\n    else:\n        stem_leaf[stem] = [leaf]\n\n# Print stem and leaf plot\nfor stem, leaves in sorted(stem_leaf.items()):\n    print(f\"{stem} | {' '.join(map(str, sorted(leaves)))}\")\n\n2 | 0\n3 | 4 4 6 6\n4 | 5\n5 | 2 3 6 7 9\n6 | 0 1 2 3 4 5 6 7 8 9\n7 | 3 4 4 5 6 8 9\n8 | 0 1 5\n9 | 0 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html",
    "href": "ParameterEstimation.html",
    "title": "2  Parameter Estimation",
    "section": "",
    "text": "2.1 Point Estimation: Maximum Likelihood Estimators\n(Please refer the book titled “Introduction to Probability and Statistics for Engineers and Scientists” by Sheldon M Ross for more details)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html#interval-estimates",
    "href": "ParameterEstimation.html#interval-estimates",
    "title": "2  Parameter Estimation",
    "section": "2.2 Interval Estimates",
    "text": "2.2 Interval Estimates\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a known distribution with an unknown mean \\(\\mu\\).\nIt is established that the sample mean \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) serves as the maximum likelihood estimator for \\(\\mu\\).\nHowever, the sample mean \\(\\bar{X}\\) is not expected to be exactly equal to \\(\\mu\\), but rather close to it.\nTherefore, instead of providing a single point estimate, it is often more useful to specify an interval within which we are confident that \\(\\mu\\) lies.\nTo determine such an interval estimator, we utilize the probability distribution of the point estimator.\n\n\n2.2.1 Confidence Intervals for the Mean of a normal population with known Variance\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nThe point estimator \\(\\bar{X}\\) is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\)/n.\nTherefore, \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\) follows a standard normal distribution.\n\n\n\n\n\n\n\nWhat to do\n\n\n\nConsider that I want to find an interval around \\(\\bar{X}\\) such that the actual population mean \\(\\mu\\) falls within the interval, say 95 % of the times.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor finding such an interval, I can use the Z-table. From the Z-table I can find:\n\n\\[\nP\\left( -1.96 &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96 \\right) = 0.9750 - 0.0250 = 0.95\n\\]\n\nRewriting the above equation:\n\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} -\\mu &lt; 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( 1.96\\frac{\\sigma}{\\sqrt{n}} &gt; \\mu - \\bar{X} &gt; -1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu - \\bar{X} &lt; 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\nWe have P(Z &lt; -1.96) = 0.025, similarly P(Z &gt; 1.96) = 0.025. Usually 1.96 is represented generally as \\(z_{0.025}\\). Thus, P(Z &lt; -z\\(_{0.025}\\)) = 0.025 and P(Z &gt; z\\(_{0.025}\\)) = 0.025.\nHence, 100(1-0.05) percent confidence interval for the mean of a normal population with known variance is:\n\n\\[\nP\\left( \\bar{X} - z_{0.025}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}}\\right) = (1 - 0.05)\n\\]\n\n\n\nFor a confidence level of \\(100(1-\\alpha)\\) percent, the corresponding critical value from the standard normal distribution is \\(z_{\\alpha/2}\\).\nThe \\(100(1-\\alpha)\\) percent confidence interval for \\(\\mu\\) is given by:\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.1}\\]\n\nThe interval as given in Equation 2.1 is called a two-sided confidence interval.\nAlso the term \\(z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\) is called the margin of error.\n\n\n\n\n\n\n\nDerivation of two-sided confidence interval\n\n\n\n\nTo find 100(1-\\(\\alpha\\)) percent confidence interval of mean (\\(\\mu\\)), we have;\n\n\\[\nP\\left( -z_{\\alpha/2} &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; z{_\\alpha/2} \\right) = 1 - \\alpha\n\\]\n\nDoing the same manipulations we did earlier for obtaining the 95percent confidence interval we can obtain:\n\n\\[\nP\\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nThe above equation give us the required confidence interval, as given in Equation 2.1.\n\n\n\n\n\n\n\n\n\nWhat if !?\n\n\n\nWhat if we are interested in one sided confidence intervals !!?\n\n\n\n\n\n\n\n\nOne-sided Upper Confidence Iterval\n\n\n\n\nTo determine such an interval, for a standard normal random variable Z, we have;\n\n\\[\nP\\left( Z &lt; 1.645 \\right) = 0.95\n\\]\n\nThus, \\[\nP\\left( \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} &lt; 1.645 \\right) = 0.95\n\\]\n\n\\[\nP\\left( \\mu -\\bar{X} &gt; - 1.645\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}  \\right) = 0.95\n\\]\n\nThus a 95 percent one-sided upper confidence interval for \\(\\mu\\) is\n\n\\[\n\\mu \\in \\left( \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\nor in other words; 100(1-0.05) percent one-sided upper confidence interval for \\(\\mu\\) is\n\\[\n\\mu \\in \\left( \\bar{X} - z_{0.05}\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\n\n\n\n\n\n\n\n\nOneside interval!\n\n\n\nCan you think of another one sided confidence interval?\n\n\n\n\n\n\n\n\nOne-sided lower confidence interval\n\n\n\n\nWe have \\[\nP\\left( Z &gt; - 1.645 \\right) = 0.95\n\\]\nProceed just like in the previous case and you will find a 100(1-0.05) percent one-sided lower confidence interval for \\(\\mu\\) as;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{0.05}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\]\n\n\n\nIn general, 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval for \\(\\mu\\) is given in Equation 2.2.\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}, \\infty \\right)\n\\tag{2.2}\\]\n\nAlso, 100(1-\\(\\alpha\\))percent one-sided lower confidence interval for \\(\\mu\\) is given in Equation 2.3. \\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.3}\\]\nThe python code below creates a sample and find 95% confidence interval for the mean if the population standard deviation is assumed to be 10. Other values are specified in the code.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 50  # true mean\nsigma = 10  # known standard deviation\nn = 30  # sample size\nalpha = 0.05  # significance level\n\n# Generate a sample\nnp.random.seed(0)\nsample = np.random.normal(mu, sigma, n)\nsample_mean = np.mean(sample)\n\n# Calculate the confidence interval\nz = 1.96  # z-value for 95% confidence\nmargin_of_error = z * (sigma / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\n# Plot the sample and confidence interval\nplt.figure(figsize=(8, 4))\nplt.hist(sample, bins=10, alpha=0.7, color='blue', edgecolor='black')\nplt.axvline(sample_mean, color='red', linestyle='dashed', linewidth=2, label='Sample Mean')\nplt.axvline(confidence_interval[0], color='green', linestyle='dashed', linewidth=2, label='95% CI Lower Bound')\nplt.axvline(confidence_interval[1], color='green', linestyle='dashed', linewidth=2, label='95% CI Upper Bound')\nplt.title('Sample Distribution with 95% Confidence Interval')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"95% Confidence Interval: {confidence_interval}\")\n\n\n\n\n\n\n\n\nSample Mean: 54.42856447263174\n95% Confidence Interval: (50.85011043026466, 58.007018514998826)\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose that when a signal having value \\(\\mu\\) is transmitted from location A the value received at location B is normally distributed with mean \\(\\mu\\) and variance 4. That is, if \\(\\mu\\) is sent, then the value received is \\(\\mu\\) + N where N, representing noise, is normal with mean 0 and variance 4. To reduce error, suppose the same value is sent 9 times. If the successive values received are 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5;\n(a). construct a 95 percent two-sided confidence interval for \\(\\mu\\).\n(b). construct 95 percent one-sided upper and lower confidence intervals for \\(\\mu\\).\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose a quality control manager at a factory wants to ensure that the average weight of a product is at least 500 grams. They take a random sample of 30 products and find the sample mean weight to be 495 grams with a standard deviation of 10 grams. Help the manager to estimate the minimum average weight of the products with 95% confidence.\n\n\n\n\n2.2.2 Confidence Intervals for the Mean of a normal population with unknown Variance\n\nIf you recollect the discussion we had about the sample mean from a normal population with unknown variance we saw that variable t\\(_{n-1}\\) given by:\n\n\\[\nt_{n-1} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{S}\n\\]\nhas a t-distribution with n-1 degrees of freedom.\n\nBecause of the symmetry of the t-distribution we can write for any \\(\\alpha\\) \\(\\in\\) (0, 1/2);\n\n\\[\nP\\left( -t_{\\alpha/2, n-1} &lt; \\sqrt{n}\\frac{\\bar{X} - \\mu}{S} &lt;  t_{\\alpha/2, n-1} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( -\\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &lt;  - \\mu &lt; -\\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &gt;   \\mu &gt; \\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nIf the sample mean is \\(\\bar{X}\\) and sample standard deviation S, then we can say that with 100(1-\\(\\alpha\\)) percent confidence that\n\n\\[\n\\mu \\in \\left(\\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}},  \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} \\right)\n\\]\n\nIn this case 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval can be obtained from the fact that:\n\n\\[\nP\\left( \\sqrt{n}\\frac{(\\bar{X} - \\mu)}{S} &lt; t_{\\alpha, n-1}\\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right) = 1 - \\alpha  \n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided upper confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}, \\infty \\right)\n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided lower confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nLet us again consider the previous problem but let us now suppose that when the value \\(\\mu\\) is transmitted at location A then the value received at location B is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) but with \\(\\sigma^2\\) being unknown. If 9 successive values are, 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, and 10.5, compute a 95 percent confidence interval for \\(\\mu\\).\n\n\n\n\n2.2.3 Confidence Intervals for the Variance of a Normal Distribution\n\nIf we are sampling from a normal distribution with unknown mean and unknown variance then;\n\n\\[\n(n-1) \\frac{S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nfollows a chi-squared distribution.\n\nWe have\n\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}  \\right) = 1 - \\alpha\n\\]\n\nHence, 100(1-\\(\\alpha\\)) percent two-sided confidence interval for the variance in this case;\n\n\\[\n\\sigma^2 \\in \\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\n\\]\n\nThe 100(1-\\(\\alpha\\)) percent one-sided upper and lower confidence intervals in this case will be respectively;\n\n\\[\n\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha, n-1}}, \\infty \\right)\n\\]\nand\n\\[\n\\left( 0, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha, n-1}} \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nA standardized procedure is expected to produce washers with very small deviation in their thicknesses. Suppose that 10 such washers were chosen and measured. If the thicknesses of these washers were, in inches; .123, .133, .124, .125, .126, .128, .120, .124, .130, and .126. What is a 90 percent confidence interval for the standard deviation of the thickness of a washer produced by this procedure?\n\n\nAll problems and most part of text are taken from Ross (2009) .\n\n\n\n\nRoss, Sheldon. 2009. “Probability and Statistics for Engineers and Scientists.” Elsevier, New Delhi 16: 32–33.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html",
    "href": "HypothesisTesting.html",
    "title": "3  Hypothesis Testing",
    "section": "",
    "text": "Introduction\n\\[\nH_{0}: \\theta = 1\n\\]\n\\[\nH_{0}: \\theta &gt; 1\n\\]\n\\[\nH_{0}: \\theta \\leq 1\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html#with-known-variance-z-test",
    "href": "HypothesisTesting.html#with-known-variance-z-test",
    "title": "3  Hypothesis Testing",
    "section": "4.1 With known Variance (Z-test)",
    "text": "4.1 With known Variance (Z-test)\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nWe are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nSince \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) is a natural point estimator of \\(\\mu\\), it is reasonable to accept \\(H_0\\) if \\(\\bar{X}\\) is not too far from \\(\\mu_0\\).\nThus, the critical region of the test would be of the form:\n\n\\[\nC = \\{X_1, \\ldots, X_n : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nfor some suitably chosen value \\(c\\).\n\nTo ensure that the test has a significance level \\(\\alpha\\), we must determine the critical value \\(c\\) in the above equation such that the type I error is equal to \\(\\alpha\\). This means \\(c\\) must satisfy:\n\n\\[\nP_{\\mu_0} \\{|\\bar{X} - \\mu_0| &gt; c\\} = \\alpha\n\\]\nwhere \\(P_{\\mu_0}\\) denotes that the probability is computed under the assumption that population mean, \\(\\mu = \\mu_0\\).\n\nWhen \\(\\mu = \\mu_0\\), \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\frac{\\sigma^2}{n}\\). Therefore, the standardized variable \\(Z\\) defined by:\n\n\\[\nZ = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n\\]\nwill have a standard normal distribution.\n\nThe probability of a type I error is given by:\n\n\\[\nP \\left( |\\bar{X} - \\mu_0| &gt; c \\right) = \\alpha\n\\]\n\nEquivalently, this can be written as:\n\n\\[\n2P \\left( Z &gt; \\frac{c \\sqrt{n}}{\\sigma} \\right) = \\alpha\n\\]\n\nWhere \\(Z\\) is a standard normal random variable. We know that:\n\n\\[\nP \\left( Z &gt; z_{\\alpha/2} \\right) = \\frac{\\alpha}{2}\n\\]\n\nTherefore, we have:\n\n\\[\n\\frac{c \\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\n\nSolving for \\(c\\), we get:\n\n\\[\nc = \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nThus, the test at significance level \\(\\alpha\\) is to reject \\(H_0\\) if:\n\n\\[\n|\\bar{X} - \\mu_0| &gt; \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nAnd accept \\(H_0\\) otherwise. Equivalently, we can reject \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} &gt; z_{\\alpha/2}\n\\]\n\nAnd accept \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} \\leq z_{\\alpha/2}\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nIf a signal of value \\(\\mu\\) is sent from location A, then the value received at location B is normally distributed with mean \\(\\mu\\) and standard deviation 2. That is, the random noise added to the signal is an N(0, 4) random variable. There is reason for the people at location B to suspect that the signal value \\(\\mu\\) = 8 will be sent today. Test this hypothesis if the same signal value is independently sent five times and the average value received at location B is X = 9. 5.\n\n\n\n4.1.1 Choosing the Significance Level\n\nThe appropriate significance level \\(\\alpha\\) depends on the specific context and consequences of the hypothesis test.\nIf rejecting the null hypothesis \\(H_0\\) would lead to significant costs or consequences, a more conservative significance level (e.g., 0.05 or 0.01) should be chosen.\nIf there is a strong initial belief that \\(H_0\\) is true, strict evidence is required to reject \\(H_0\\), implying a lower significance level.\nThe test can be described as follows: For an observed value of the test statistic \\(\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma}\\), denoted as \\(v\\), reject \\(H_0\\) if the probability of the test statistic being as large as \\(v\\) under \\(H_0\\) is less than or equal to \\(\\alpha\\).\nThis probability is known as the p-value of the test. \\(H_0\\) is accepted if \\(\\alpha\\) is less than the p-value and rejected if \\(\\alpha\\) is greater than or equal to the p-value.\nIn practice, the significance level is sometimes not set in advance. Instead, the p-value is calculated from the data, and decisions are made based on the p-value.\nIf the p-value is much larger than any reasonable significance level, \\(H_0\\) is accepted. Conversely, if the p-value is very small, \\(H_0\\) is rejected.\n\n\n\n4.1.2 Hypothesis Testing Summary: Z- Test\n\nCaption: Summary of hypothesis testing for a sample from a \\(N(\\mu, \\sigma^2)\\) population with known \\(\\sigma^2\\).\n\n\n\n\n\n\nSample and Population\nDetails\n\n\n\n\nSample\n\\(\\{X_1, X_2, . . . , X_n\\}\\)\n\n\nPopulation\n\\(N(\\mu, \\sigma^2)\\)\n\n\nKnown Parameter\n\\(\\sigma^2\\)\n\n\nSample Mean\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\n\n\nSignificance Level\n\\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nTest Statistic (TS)\nReject if\np-Value if TS = t\n\n\n\n\n\\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(|TS| &gt; z_{\\alpha/2}\\)\n\\(2P\\{Z \\geq |t|\\}\\)\n\n\n\\(H_0: \\mu \\leq \\mu_0\\) vs \\(H_1: \\mu &gt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &gt; z_{\\alpha}\\)\n\\(P\\{Z \\geq t\\}\\)\n\n\n\\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu &lt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &lt; -z_{\\alpha}\\)\n\\(P\\{Z \\leq t\\}\\)\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nImagine you’re the quality control manager at a company that prides itself on the precision of its product weights. The company claims that the average weight of their product is exactly 100 grams. But, as a diligent manager, you decide to put this claim to the test. You randomly select a sample of 30 products and measure their weights. To your surprise, the average weight of your sample is 110 grams! Now, you need to determine if this difference is statistically significant or just a fluke. Assume the population standard deviation as 15 grams.\n\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Given data\nsample_mean = 495\npopulation_mean = 500\nstd_dev = 10\nsample_size = 30\nalpha = 0.05\n\n# Calculate the Z-score\nz_score = (sample_mean - population_mean) / (std_dev / np.sqrt(sample_size))\n\n# Calculate the p-value\np_value = stats.norm.cdf(z_score)\n\n# Determine if we reject the null hypothesis\nreject_null = p_value &lt; alpha\n\n# Output the results\nprint(f\"Z-score: {z_score}\")\nprint(f\"P-value: {p_value}\")\nprint(f\"Reject the null hypothesis: {reject_null}\")\n\nZ-score: -2.7386127875258306\nP-value: 0.00308494966027208\nReject the null hypothesis: True",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html#with-unknown-variance-t-test",
    "href": "HypothesisTesting.html#with-unknown-variance-t-test",
    "title": "3  Hypothesis Testing",
    "section": "4.2 With unknown Variance (T-test)",
    "text": "4.2 With unknown Variance (T-test)\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a unknown variance.\nSay, we are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nIn the previous case (with known variance), for a significance level (\\(\\alpha\\)) we accepted the null hypothesis if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0 }{\\sigma/\\sqrt{n}} \\right| \\leq z_{\\alpha/2}\n\\]\n\nBut in this case, \\(\\sigma\\) is unknown.\nWe know that the statistic, T, as given below has a t-distribution with n-1 degrees of freedom when \\(\\mu\\) = \\(\\mu_0\\).\n\n\\[\nT = \\frac{\\bar{X} - \\mu_0 }{S\\sqrt{n}}\n\\]\nwhere S is the sample standard deviation.\n\nHence here with \\(H_0\\): \\(\\mu\\) = \\(\\mu_0\\) and \\(H_1\\); \\(\\mu\\) \\(\\neq\\) \\(\\mu_0\\); analogous to the Z-test here in T-test we can:\n\n\n\n\n\n\n\ntwo-sided t-test\n\n\n\n\nreject the null hypothesis (\\(H_0\\)) if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} \\right| &gt; t_{\\alpha/2}\n\\]\n\naccept \\(H_0\\) if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} \\right|  \\leq t_{\\alpha/2}\n\\]\n\n\n\n4.2.1 Hypothesis Testing Summary: T- Test\n\nCaption: Summary of hypothesis testing for a sample from a \\(N(\\mu, \\sigma^2)\\) population with unknown \\(\\sigma^2\\).\n\n\n\n\n\n\nSample and Population\nDetails\n\n\n\n\nSample\n\\(\\{X_1, X_2, . . . , X_n\\}\\)\n\n\nPopulation\n\\(N(\\mu, \\sigma^2)\\)\n\n\nSample Mean\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\n\n\nSample Variance\n\\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i-\\bar{X})^2\\)\n\n\nSignificance Level\n\\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nTest Statistic (TS)\nReject if\np-Value if TS = t\n\n\n\n\n\\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(|TS| &gt; t_{\\alpha/2, n-1}\\)\n\\(2P\\{T_{n-1} \\geq |t|\\}\\)\n\n\n\\(H_0: \\mu \\leq \\mu_0\\) vs \\(H_1: \\mu &gt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(TS &gt; t_{\\alpha, n-1}\\)\n\\(P\\{T_{n-1} \\geq t\\}\\)\n\n\n\\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu &lt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(TS &lt; -t_{\\alpha, n-1}\\)\n\\(P\\{T_{n-1} \\leq t\\}\\)\n\n\n\n\\(\\text{\\textcolor{gray}{$T_{n−1}$ is a t-random variable with (n - 1) degrees of freedom: P($T_{n−1}$ &gt; $t_{\\alpha,n−1}$) = $\\alpha$.}}\\)\n\n\n\n\n\n\nProblem\n\n\n\nA public health official claims that the mean home water use is at most 350 gallons a day. To verify this claim, a study of 20 randomly selected homes was instigated with the result that the average daily water uses of these 20 homes were as follows:\n340 344 362 375 356 386 354 364 332 402 340 355 362 322 372 324 318 360 338 370\nDo the data contradict the official’s claim?\n\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Given data\ndata = [340, 344, 362, 375, 356, 386, 354, 364, 332, 402, 340, 355, 362, 322, 372, 324, 318, 360, 338, 370]\nsample_mean = np.mean(data)\nsample_std = np.std(data, ddof=1)\nsample_size = len(data)\npopulation_mean = 350\nalpha = 0.05\n\n# Calculate the T-score\nt_score = (sample_mean - population_mean) / (sample_std / np.sqrt(sample_size))\n\n# Calculate the p-value\np_value = 2 * (1 - stats.t.cdf(np.abs(t_score), df=sample_size-1))\n\n# Determine if we reject the null hypothesis\nreject_null = p_value &lt; alpha\n\n# Output the results\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"Sample Standard Deviation: {sample_std}\")\nprint(f\"T-score: {t_score}\")\nprint(f\"P-value: {p_value}\")\nprint(f\"Reject the null hypothesis: {reject_null}\")\n\nSample Mean: 353.8\nSample Standard Deviation: 21.847798877449275\nT-score: 0.7778411328447066\nP-value: 0.4462410900531899\nReject the null hypothesis: False",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "Anova.html",
    "href": "Anova.html",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "4.1 Introduction\nHere, we will perform an Analysis of Variance (ANOVA) to determine if there are any statistically significant differences between the means of three or more independent groups.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nX = np.array([10, 20, 30, 40])\nY = np.array([12, 21, 34, 39])\nZ = np.array([8, 11, 31, 39])\n\n# Grand mean\ngrand_mean = np.mean(np.concatenate([X, Y, Z]))\n\n# Scatter plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.scatter(np.ones_like(X), X, label='X')\nplt.scatter(np.ones_like(Y) * 2, Y, label='Y')\nplt.scatter(np.ones_like(Z) * 3, Z, label='Z')\nplt.xticks([1, 2, 3], ['X', 'Y', 'Z'])\nplt.xlabel('Samples')\nplt.ylabel('Values')\nplt.title('Scatter Plot of Samples')\nplt.axhline(grand_mean, color='red', linestyle='--', label='Grand Mean')\nplt.legend()\n\n# Box plot\nplt.subplot(1, 2, 2)\nplt.boxplot([X, Y, Z], labels=['X', 'Y', 'Z'])\nplt.xlabel('Samples')\nplt.ylabel('Values')\nplt.title('Box Plot of Samples')\nplt.axhline(grand_mean, color='red', linestyle='--', label='Grand Mean')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nX = np.array([10, 20, 30, 40])\nY = np.array([12, 21, 34, 39])\nZ = np.array([8, 11, 31, 39])\n\n# Means\nmean_X = np.mean(X)\nmean_Y = np.mean(Y)\nmean_Z = np.mean(Z)\n\n# Grand mean\ngrand_mean = np.mean(np.concatenate([X, Y, Z]))\n\n# Scatter plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(2, 1, 1)\nplt.scatter(np.ones_like(X), X, label='X')\nplt.scatter(np.ones_like(Y) * 2, Y, label='Y')\nplt.scatter(np.ones_like(Z) * 3, Z, label='Z')\nplt.scatter([1], [mean_X], label='Mean of X')\nplt.scatter([2], [mean_Y], label='Mean of Y')\nplt.scatter([3], [mean_Z], label='Mean of Z')\nplt.xticks([1, 2, 3], ['X', 'Y', 'Z'])\nplt.xlabel('Samples')\nplt.ylabel('Means')\nplt.title('Scatter Plot of Sample Means')\n#plt.axhline(grand_mean, color='red', linestyle='--', label='Grand Mean')\n#plt.legend(outside)\nplt.legend(loc='center right', bbox_to_anchor=(1.25, 0.5))\n\n\n# Box plot\nplt.subplot(2, 1, 2)\nplt.boxplot([X, Y, Z], labels=['X', 'Y', 'Z'])\nplt.boxplot([[mean_X], [mean_Y], [mean_Z]], labels=['X', 'Y', 'Z'])\nplt.xlabel('Samples')\nplt.ylabel('Means')\nplt.title('Box Plot of Sample Means')\n#plt.axhline(grand_mean, color='red', linestyle='--', label='Grand Mean')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Anova.html#introduction",
    "href": "Anova.html#introduction",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Comparison of means of three samples\n\n\n\n\nConsider three independent samples:\n\nX: 10, 20, 30, 40\nY: 12, 21, 34, 39\nZ: 8, 11, 31, 39\n\nWe want to test if the means of these three samples are significantly different from each other.\nCalculate the mean of each sample:\n\nMean of X: \\((10 + 20 + 30 + 40) / 4 = 25\\)\nMean of Y: \\((12 + 21 + 34 + 39) / 4 = 26.5\\)\nMean of Z: \\((8 + 11 + 31 + 39) / 4 = 22.25\\)\n\nCalculate the overall mean (grand mean) of all the samples combined:\n\nGrand Mean: \\((10 + 20 + 30 + 40 + 12 + 21 + 34 + 39 + 8 + 11 + 31 + 39) / 12 = 24.58\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Anova.html#one-way-analysis-of-variance",
    "href": "Anova.html#one-way-analysis-of-variance",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "4.2 One-way Analysis of Variance",
    "text": "4.2 One-way Analysis of Variance\n\nConsider m independent samples, each of size n.\n\nThe members of the ith sample are denoted as:\n\\(X_{i1}, X_{i2}, \\dots, X_{in}\\).\nEach \\(X_{ij}\\) is a normal random variable with:\n\nUnknown mean: \\(\\mu_i\\).\nUnknown variance: \\(\\sigma^2\\).\n\nMathematically:\n\\(X_{ij} \\sim N(\\mu_i, \\sigma^2)\\), where:\n\n\\(i = 1, \\dots, m\\).\n\\(j = 1, \\dots, n\\).\n\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)):\n\\(\\mu_1 = \\mu_2 = \\dots = \\mu_m\\).\n(All population means are equal.)\nAlternative Hypothesis (\\(H_1\\)):\nNot all means are equal.\n(At least two means differ.)\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\n\nImagine m different treatments.\nApplying treatment i to an item results in a normal random variable with:\n\nMean: \\(\\mu_i\\).\nVariance: \\(\\sigma^2\\).\n\nGoal: Test if all treatments have the same effect.\nMethod:\n\nApply each treatment to a different sample of n items.\nAnalyze the results to compare the means.\n\n\n\n\n\nSince there are a total of nm independent normal random variables \\(X_{ij}\\):\n\nThe sum of the squares of their standardized versions follows a chi-square distribution with nm degrees of freedom.\nMathematically:\n\\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - E[X_{ij}])^2}{\\sigma^2} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\mu_i)^2}{\\sigma^2} \\sim \\chi^2_{nm}\n\\tag{4.1}\\]\n\nTo estimate the m unknown parameters \\(\\mu_1, \\dots, \\mu_m\\):\n\nLet \\(X_{i}\\) denote the sample mean of the ith sample: \\[\n\\bar{X}_{i} = \\sum_{j=1}^{n} \\frac{X_{ij}}{n}\n\\]\nHere, \\(\\bar{X}_{i}\\) is the estimator of the population mean \\(\\mu_i\\) for \\(i = 1, \\dots, m\\).\n\nSubstituting the estimators \\(\\bar{X}_{i}\\) for \\(\\mu_i\\) in Equation Equation 4.1:\n\nThe resulting variable: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\bar{X}_{i})^2}{\\sigma^2}\n\\tag{4.2}\\] follows a chi-square distribution with nm − m degrees of freedom.\n(One degree of freedom is lost for each estimated parameter.)\n\nDefine: \\[\nSSW = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (X_{ij} - \\bar{X}_{i})^2\n\\]\n\nThe variable in Equation 4.2 becomes \\(\\frac{SSW}{\\sigma^2}\\).\n\nSince the expected value of a chi-square random variable equals its degrees of freedom:\n\nTaking the expectation of the variable in Equation Equation 4.2 gives: \\[\nE\\left[\\frac{SSW}{\\sigma^2}\\right] = nm - m\n\\]\nEquivalently: \\[\nE\\left[\\frac{SSW}{nm - m}\\right] = \\sigma^2\n\\]\n\nWe thus have our first estimator of \\(\\sigma^2\\), namely, SSW /(nm - m). The statistic SSW is called the Sum of Squares Within Samples aka ( within samples sum of squares or sum of squares within groups).\nAlso, note that this estimator was obtained without assuming anything about the truth or falsity of the null hypothesis.\nOur second estimator of \\(\\sigma^2\\) is valid only when the null hypothesis is true.\n\nAssume \\(H_0\\) is true, meaning all population means \\(\\mu_i\\) are equal, i.e., \\(\\mu_i = \\mu\\) for all \\(i\\).\nUnder this assumption:\n\nThe sample means \\(\\bar{X}_1, \\bar{X}_2, \\dots, \\bar{X}_m\\) are normally distributed with:\n\nMean: \\(\\mu\\).\nVariance: \\(\\frac{\\sigma^2}{n}\\).\n\n\n\nWe have; \\[\n\\frac{\\bar{X}_{i.} - \\mu}{\\sqrt{\\sigma^2/n}} = \\frac{\\sqrt{n}(\\bar{X}_{i.} - \\mu)}{\\sigma}\n\\] follows a standard normal distribution; hence, \\[\n  n \\sum_{i=1}^{m} \\frac{(\\bar{X}_{i.} - \\mu)^2}{\\sigma^2} \\sim \\chi^2_m\n   \\tag{4.3}\\] follows a chi-square distribution with m degrees of freedom when \\(H_0\\) is true.\nWhen all population means are equal to \\(\\mu\\), the estimator of \\(\\mu\\) is the average of all nm data values, denoted as \\(\\bar{X}_{..}\\): \\[\n\\bar{X}_{..} = \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}}{nm} = \\frac{\\sum_{i=1}^{m} \\bar{X}_{i.}}{m}\n\\]\nSubstituting \\(\\bar{X}_{..}\\) for the unknown parameter \\(\\mu\\) in Equation 4.3:\n\nWhen \\(H_0\\) is true, the resulting quantity: \\[\nn \\sum_{i=1}^{m} \\frac{(\\bar{X}_{i.} - \\bar{X}_{..})^2}{\\sigma^2}\n\\]\n\nThe resulting quantity: \\[\nn \\sum_{i=1}^{m} \\frac{(\\bar{X}_{i.} - \\bar{X}_{..})^2}{\\sigma^2}\n\\] will be a chi-square random variable with m - 1 degrees of freedom.\nDefine SSb as: \\[\nSSb = n \\sum_{i=1}^{m} (\\bar{X}_{i.} - \\bar{X}_{..})^2\n\\tag{4.4}\\]\n\nIt follows that, when \\(H_0\\) is true: \\[\nSSb / \\sigma^2 \\sim \\chi^2_{m-1}\n\\]\n\nFrom the above, we derive that when \\(H_0\\) is true: \\[\nE[SSb] / \\sigma^2 = m - 1\n\\]\n\nEquivalently: \\[\nE[SSb / (m - 1)] = \\sigma^2\n\\tag{4.5}\\]\n\nTherefore, when \\(H_0\\) is true, \\(SSb / (m - 1)\\) is also an estimator of \\(\\sigma^2\\).\nThe quantity SSb Equation 4.4 is called the sum of squares between samples or between samples sum of squares or sum of squares between groups.\nNote that \\(E[SSb / (m - 1)]\\) is an estimate of \\(\\sigma^2\\) only if the hypothesis is true.\nThus we have shown that;\n\n\\(\\frac{SSW}{nm-m}\\) always estimates \\(\\sigma^2\\).\n\\(\\frac{SSb}{m-1}\\) estimates \\(\\sigma^2\\) when \\(H_0\\) is true.\n\nBecause it can be shown that \\(\\frac{SSb}{m-1}\\) will tend to exceed \\(\\sigma^2\\) when \\(H_0\\) is not true, now it is reasonable to let the test statistic be given by\n\n\\[\nTS= \\frac{\\frac{SSb}{m-1} } {\\frac{SSW}{nm-m}}\n\\]\nand to reject \\(H_0\\) when \\(TS\\) is sufficiently large.\n\nHow to do the hypothesis testing\n\nTo determine how large \\(TS\\) needs to be to justify rejecting \\(H_0\\), we use the fact that:\n\nIf \\(H_0\\) is true, then \\(SSb\\) and \\(SSW\\) are independent.\nIt follows that, when \\(H_0\\) is true, \\(TS\\) has an \\(F\\)-distribution with \\(m-1\\) numerator and \\(nm-m\\) denominator degrees of freedom.\nLet \\(F_{m-1,nm-m,\\alpha}\\) denote the \\(100(1-\\alpha)\\) percentile of this distribution — that is, \\(P\\{F_{m-1,nm-m} &gt; F_{m-1,nm-m,\\alpha}\\} = \\alpha\\).\nWe use the notation \\(F_{r,s}\\) to represent an \\(F\\)-random variable with \\(r\\) numerator and \\(s\\) denominator degrees of freedom.\n\nThe significance level \\(\\alpha\\) test of \\(H_0\\) is as follows:\n\nReject \\(H_0\\) if \\(\\frac{\\frac{SSb}{m-1} }{ \\frac{SSW}{nm-m}} &gt; F_{m-1,nm-m,\\alpha}\\).\nDo not reject \\(H_0\\), otherwise.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe following algebraic identity, known as the sum of squares identity, is useful for simplifying computations, especially when done by hand:\nThe Sum of Squares Identity: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}^2 = nm \\bar{X}_{..}^2 + SSb + SSW\n\\]\n\nHere:\n\n\\(\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}^2\\) represents the total sum of squares of all observations.\n\\(nm \\bar{X}_{..}^2\\) is the contribution from the grand mean (\\(\\bar{X}_{..}\\)).\n\\(SSb\\) (Sum of Squares Between) measures the variation between sample means.\n\\(SSW\\) (Sum of Squares Within) measures the variation within each sample.\n\n\nThis identity helps decompose the total variability in the data into components that can be analyzed separately.\nWhen performing calculations by hand, the quantity SSb (Sum of Squares Between) should be computed first. It is defined as: \\[\nSSb = n \\sum_{i=1}^{m} (\\bar{X}_{i.} - \\bar{X}_{..})^2\n\\]\nOnce SSb has been calculated, SSW (Sum of Squares Within) can be determined using the sum of squares identity. To do this:\n\nCompute the total sum of squares: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}^2\n\\]\nCompute the term involving the grand mean: \\[\nnm \\bar{X}_{..}^2\n\\]\nUse the sum of squares identity to find SSW: \\[\nSSW = \\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}^2 - nm \\bar{X}_{..}^2 - SSb\n\\]\n\nThis approach simplifies the computation process by breaking it into manageable steps and leveraging the sum of squares identity.\n\n\n\n\n\n\n\n\n\nAlternative Method\n\n\n\nAnother useful identity involving the sum of squares is:\nError Sum of Squares (\\(SS_{total}\\)): \\(SS_{total}=SSb + SSW\\)\nwhere Total Sum of Squares (\\(SS_{total}\\)): \\(SS_{total}=\\sum_{i=1}^m\\sum_{j=1}^n(X_{ij}-\\bar{X}_{..})^2\\)\n\nThis identity can also be used for the anova analysis while performing the computations by hand.\nThis approach too simplifies the computation process by breaking it into manageable steps and leveraging the sum of squares identity.\n\n\n\n\n\nSummary of one-way analysis of variance\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{Source of Variation} & \\textbf{Sum of Squares} & \\textbf{Degrees of Freedom} \\\\\n\\hline\n\\text{Between samples} & SS_b = n \\sum_{i=1}^m (\\bar{X}_{i.} - \\bar{X}_{..})^2 & m - 1 \\\\\n\\hline\n\\text{Within samples} & SS_W = \\sum_{i=1}^m \\sum_{j=1}^n (X_{ij} - \\bar{X}_{i.})^2 & nm - m \\\\\n\\hline\n\\end{array}\n\\]\nValue of Test Statistic, \\(TS = \\frac{SS_b/(m-1)}{SS_W/(nm-m)}\\)\nSignificance level \\(\\alpha\\) test:\nreject \\(H_0\\) if \\(TS \\geq F_{m-1,nm-m,\\alpha}\\)\ndo not reject otherwise\nIf \\(TS = v\\), then \\(p\\)-value = \\(P\\{F_{m-1,nm-m} \\geq v\\}\\)\n\n\n\n\n\n\nProblem\n\n\n\nAn auto rental firm is using 15 identical motors that are adjusted to run at a fixed speed to test 3 different brands of gasoline. Each brand of gasoline is assigned to exactly 5 of the motors. Each motor runs on 10 gallons of gasoline until it is out of fuel. The following data represents the total mileages achieved by different motors using three types of gas:\n\nGas 1: \\(220, 251, 226, 246, 260\\)\nGas 2: \\(244, 235, 232, 242, 225\\)\nGas 3: \\(252, 272, 250, 238, 256\\)\n\nTest the hypothesis that the average mileage is not affected by the type of gas used. (In other words, determine if there is a significant difference in the mean mileages for the three types of gas.)\n\n\n\n\nMultiple Comparisons of Sample Means\n\nWhen the null hypothesis of equal means is rejected, we are often interested in comparing the different sample means \\(\\mu_1, \\dots, \\mu_m\\).\nOne commonly used procedure for this purpose is known as the T-method.\nFor a specified significance level \\(\\alpha\\), this method provides joint confidence intervals for all \\(\\binom{m}{2}\\) differences \\(\\mu_i - \\mu_j\\) (where \\(i \\neq j\\), and \\(i, j = 1, \\dots, m\\)), ensuring that with probability \\(1 - \\alpha\\), all confidence intervals will contain their respective differences \\(\\mu_i - \\mu_j\\).\nThe T-method is based on the following result:\nWith probability \\(1 - \\alpha\\), for every \\(i \\neq j\\): \\[\nX_{i.} - X_{j.} - W &lt; \\mu_i - \\mu_j &lt; X_{i.} - X_{j.} + W\n\\]\n\nWhere: - \\(X_{i.}\\) and \\(X_{j.}\\) are the sample means for groups \\(i\\) and \\(j\\), respectively. - \\(W\\) is the critical value derived from the Studentized range distribution, adjusted for multiple comparisons.\nwhere \\[\nW = \\sqrt{\\frac{1}{n}} \\cdot C(m, nm - m, \\alpha) \\cdot \\sqrt{\\frac{SSW}{(nm - m)}}\n\\] and the values of \\(C(m, nm - m, \\alpha)\\) are provided for \\(\\alpha = 0.05\\) and \\(\\alpha = 0.01\\).\n\n\n\n\n\n\nProblem\n\n\n\nA college administrator claims that there is no difference in first-year grade point averages for students entering the college from any of three different city high schools. The following data provide the first-year grade point averages of 12 randomly chosen students, 4 from each of the three high schools. At the 5 percent level of significance, do these data disprove the administrator’s claim? If so, determine confidence intervals for the difference in means of students from the different high schools, such that we can be 95 percent confident that all of the interval statements are valid.\nSchool 1: 3.2, 3.4, 3.3, 3.5\nSchool 2: 3.4, 3.0, 3.7, 3.3\nSchool 3: 2.8, 2.6, 3.0, 2.7\n\n\n\n\n4.2.1 One-Way Analysis of Variance with Unequal Sample Sizes\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n\\textbf{Source of Variation} & \\textbf{Sum of Squares} & \\textbf{Degrees of Freedom} \\\\\n\\hline\n\\text{Between samples} & SS_b =  \\sum_{i=1}^m n_i (\\bar{X}_{i.} - \\bar{X}_{..})^2 & m - 1 \\\\\n\\hline\n\\text{Within samples} & SS_W = \\sum_{i=1}^m \\sum_{j=1}^n (X_{ij} - \\bar{X}_{i.})^2 & N - m \\\\\n\\hline\n\\end{array}\n\\]\nwhere, N = \\(\\sum_{i=1}^m n_i\\)\nValue of Test Statistic, \\(TS = \\frac{SS_b/(m-1)}{SS_W/(N-m)}\\)\nSignificance level \\(\\alpha\\) test:\nreject \\(H_0\\) if \\(TS \\geq F_{m-1,N-m,\\alpha}\\)\ndo not reject otherwise\nIf \\(TS = v\\), then \\(p\\)-value = \\(P\\{F_{m-1,N-m} \\geq v\\}\\)\n\n\n\n\n\n\nProblem\n\n\n\nTest the hypothesis that the following three independent samples are drawn from the same normal probability distribution.\nSample 1: \\(35, 37, 29, 27, 30\\)\nSample 2: \\(29, 38, 34, 30, 32\\)\nSample 3: \\(44, 52, 56\\)\nUse a statistical test to determine whether the means of these samples are significantly different.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Anova.html#two-way-analysis-of-variance",
    "href": "Anova.html#two-way-analysis-of-variance",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "4.3 Two-way Analysis of Variance",
    "text": "4.3 Two-way Analysis of Variance\n\nIntroduction\n\n\n\n\n\n\nExample\n\n\n\nConsider that four different examinations were administered to each of 5 students, with the scores shown in the table below. Each of the 20 data points is influenced by two factors: the exam and the student whose score on that exam is being recorded.\n\nThe exam factor has 4 possible levels.\nThe student factor has 5 possible levels.\nData Table: \\[\n\\begin{array}{c|ccccc}\n\\text{Exam} & \\text{Student 1} & \\text{Student 2} & \\text{Student 3} & \\text{Student 4} & \\text{Student 5} \\\\\n\\hline\n1 & 75 & 73 & 60 & 70 & 86 \\\\\n2 & 78 & 71 & 64 & 72 & 90 \\\\\n3 & 80 & 69 & 62 & 70 & 85 \\\\\n4 & 73 & 67 & 63 & 80 & 92 \\\\\n\\end{array}\n\\]\n\n\n\n\nIn general, suppose there are m possible levels of the first factor (row factor) and n possible levels of the second factor (column factor). Let \\(X_{ij}\\) denote the value obtained when:\n\nThe first factor is at level \\(i\\).\nThe second factor is at level \\(j\\).\n\nThe data can be represented in an array format: \\[\n\\begin{array}{cccccc}\nX_{11} & X_{12} & \\dots & X_{1j} & \\dots & X_{1n} \\\\\nX_{21} & X_{22} & \\dots & X_{2j} & \\dots & X_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nX_{i1} & X_{i2} & \\dots & X_{ij} & \\dots & X_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\nX_{m1} & X_{m2} & \\dots & X_{mj} & \\dots & X_{mn} \\\\\n\\end{array}\n\\]\nHere, the first factor is referred to as the row factor, and the second factor is referred to as the column factor.\nAs in the case of one-way analysis assume that the data \\(X_{ij}\\) (\\(i = 1, \\dots, m\\), \\(j = 1, \\dots, n\\)) are independent normal random variables with a common variance \\(\\sigma^2\\). However, unlike the one-way analysis, where only a single factor affected the mean value of a data point, we now assume that the mean value of the data depends additively on both its row and column factors.\n\n\n\nSummary of Two-way analysis of variance\n\n\n\n\n\n\n\nSum of Squares\nDegrees of Freedom\n\n\n\n\nRow\n\\(SS_r = n\\sum_{i=1}^m (\\bar{X}_{i.} - \\bar{X}_{..})^2\\)\n\n\nColumn\n\\(SS_c = m\\sum_{j=1}^n (\\bar{X}_{.j} - \\bar{X}_{..})^2\\)\n\n\nError\n\\(SS_e = \\sum_{i=1}^m \\sum_{j=1}^n (X_{ij} - \\bar{X}_{i.} - \\bar{X}_{.j} + \\bar{X}_{..})^2\\)\n\n\n\nLet \\(N = (m - 1)(n - 1)\\)\n\n\n\n\n\n\n\n\n\nNull Hypothesis\nTest Statistic\nSignificance Level \\(\\alpha\\) Test\np-value if TS = \\(v\\)\n\n\n\n\nRow factor has no effect\n\\(\\frac{SS_r/(m - 1)}{SS_e/N}\\)\nReject if \\(TS \\geq F_{m-1,N,\\alpha}\\)\n\\(P\\{F_{m-1,N} \\geq v\\}\\)\n\n\nColumn factor has no effect\n\\(\\frac{SS_c/(n - 1)}{SS_e/N}\\)\nReject if \\(TS \\geq F_{n-1,N,\\alpha}\\)\n\\(P\\{F_{n-1,N} \\geq v\\}\\)\n\n\n\nwhere:\n\nRow Mean (\\(\\bar{X}_i.\\)): \\[\n\\bar{X}_{i.} = \\frac{\\sum_{j=1}^n X_{ij}}{n} = \\text{Average of the values in row } i\n\\]\nColumn Mean (\\(\\bar{X}_{.j}\\)): \\[\n\\bar{X}_{.j} = \\frac{\\sum_{i=1}^m X_{ij}}{m} = \\text{Average of the values in column } j\n\\]\nGrand Mean (\\(\\bar{X}_{..}\\)): \\[\n\\bar{X}_{..} = \\frac{\\sum_{i=1}^m \\sum_{j=1}^n X_{ij}}{nm} = \\text{Average of all data values}\n\\]\n\n\n\n\n\n\n\nProblem 1\n\n\n\nA researcher aims to investigate the impact of two factors on crop yield:\n\nFactor A (Fertilizer Type): Two levels - Type 1 (\\(T1\\)) and Type 2 (\\(T2\\)).\nFactor B (Irrigation Level): Three levels - Low (\\(L\\)), Medium (\\(M\\)), and High (\\(H\\)).\n\nThe researcher measures the yield (in kg) for each combination of factors. Due to the high cost of the experiment, only one observation is recorded for each combination. The data is as follows:\n\n\n\nFertilizer\nIrrigation\nYield (kg)\n\n\n\n\nType 1 (\\(T1\\))\nLow (\\(L\\))\n55\n\n\nType 1 (\\(T1\\))\nMedium (\\(M\\))\n60\n\n\nType 1 (\\(T1\\))\nHigh (\\(H\\))\n70\n\n\nType 2 (\\(T2\\))\nLow (\\(L\\))\n55\n\n\nType 2 (\\(T2\\))\nMedium (\\(M\\))\n65\n\n\nType 2 (\\(T2\\))\nHigh (\\(H\\))\n75\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere also we have an useful identity for the sum of squares\nTotal Sum of Squares (\\(SS_{total}\\)): \\(SS_{total}=SS_{error}+SS_r+SS_c\\)\nwhere Total Sum of Squares (\\(SS_{total}\\)): \\(SS_{total}=\\sum_{i=1}^m\\sum_{j=1}^n(X_{ij}-\\bar{X}_{..})^2\\)\n\n\n\n\nSupplementary Topics\n\n\n\n\n\n\nDegrees of Freedom (dof)\n\n\n\n\nThe statement “One degree of freedom is lost for each estimated parameter” is a key concept in statistics, especially in hypothesis testing and parameter estimation. Here’s a detailed explanation:\n\n\nWhat Are Degrees of Freedom?\n\nDegrees of freedom (df) represent the number of independent pieces of information available to estimate a parameter or test a hypothesis.\nIn simpler terms, it’s the number of values in a calculation that are free to vary after certain constraints (like estimating parameters) are applied.\n\n\n\nWhy Are Degrees of Freedom Lost?\n\nWhen you estimate parameters (e.g., population means, variances) from sample data, you use the data itself to calculate these estimates.\nEach time you estimate a parameter, you impose a constraint on the data, reducing the number of independent pieces of information available.\nExample: If you estimate the sample mean (\\(\\bar{X}\\)) from a dataset, you use the data to calculate \\(\\bar{X}\\). This means one piece of information (one degree of freedom) is “used up” in estimating \\(\\bar{X}\\), and the remaining data points are no longer fully independent.\n\n\n\nApplication in the Context of the Problem\n\nIn the given problem:\n\nYou have m samples, each of size n, and you estimate the mean of each sample (\\(\\mu_i\\)) using the sample mean (\\(\\bar{X}_{i.}\\)).\nEach time you estimate a mean (\\(\\mu_i\\)), you lose one degree of freedom because the data is used to calculate that estimate.\nSince you estimate m means (\\(\\mu_1, \\mu_2, \\dots, \\mu_m\\)), you lose m degrees of freedom in total.\n\n\n\n\nMathematical Explanation\n\nInitially, the sum of squares: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\mu_i)^2}{\\sigma^2}\n\\] follows a chi-square distribution with nm degrees of freedom (since there are nm independent observations).\nHowever, when you replace the true means (\\(\\mu_i\\)) with their estimates (\\(\\bar{X}_{i.}\\)), you lose m degrees of freedom (one for each estimated mean). This is because the estimates are derived from the data, reducing the independence of the observations.\nAs a result, the modified sum of squares: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\bar{X}_{i.})^2}{\\sigma^2}\n\\] follows a chi-square distribution with nm - m degrees of freedom.\n\n\n\nWhy Does This Matter?\n\nDegrees of freedom affect the shape and critical values of the chi-square distribution, which is used for hypothesis testing.\nLosing degrees of freedom accounts for the fact that estimating parameters introduces uncertainty into the analysis.\nThis adjustment ensures that statistical tests (e.g., ANOVA) are accurate and reliable.\n\n\n\nSummary\n\nDegrees of freedom represent independent information in the data.\nEach estimated parameter (e.g., a mean) reduces the degrees of freedom by 1 because the data is used to calculate the estimate.\nIn the problem, estimating m means reduces the degrees of freedom from nm to nm - m, ensuring the chi-square distribution is correctly applied.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "5  Regression",
    "section": "",
    "text": "5.1 Linear Regression\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Parameters for the linear regression model\nalpha = 2.0  # Intercept\nbeta = 1.5   # Slope\nnum_samples = 100  # Number of data points\n\n# Generate synthetic data\nx = np.linspace(0, 10, num_samples)  # Independent variable (input)\ne = np.random.normal(0, 1, num_samples)  # Random error with mean 0\nY = alpha + beta * x + e  # Dependent variable (response)\n\n# Plot the data points\nplt.scatter(x, Y, color='blue', label='Data Points')\n\n# Plot the true regression line (without error)\nplt.plot(x, alpha + beta * x, color='red', label='True Regression Line')\n\n# Add labels and title\nplt.xlabel('Independent Variable (x)')\nplt.ylabel('Dependent/ Response Variable (Y)')\nplt.title('Simple Linear Regression Demonstration')\nplt.legend()\n\n# Show the plot\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#linear-regression",
    "href": "Regression.html#linear-regression",
    "title": "5  Regression",
    "section": "",
    "text": "Many engineering and scientific problems aim to determine relationships between variables.\n\nExample: In a chemical process, the relationship between output, temperature, and catalyst amount is of interest.\nKnowing this relationship allows predicting outputs for different temperature and catalyst values.\n\nTypically, there is a single response variable \\(Y\\) (dependent variable) that depends on a set of input variables \\(x_1, x_2, \\dots, x_r\\) (independent variables).\nThe simplest relationship between \\(Y\\) and \\(x_1, x_2, \\dots, x_r\\) is a linear relationship: \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r\n\\tag{5.1}\\]\n\nHere, \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) are constants.\n\nIn practice, exact predictions are rarely possible due to random errors.\n\nThe relationship is better expressed as: \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r + e\n\\tag{5.2}\\]\n\n\\(e\\) represents the random error, assumed to have a mean of \\(0\\).\n\n\nAlternatively, the relationship can be expressed in terms of the expected value: \\[\nE[Y \\mid \\mathbf{x}] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r\n\\]\n\n\\(\\mathbf{x} = (x_1, x_2, \\dots, x_r)\\) is the set of independent variables.\n\\(E[Y \\mid \\mathbf{x}]\\) is the expected response given the inputs \\(\\mathbf{x}\\).\n\nThe Equation 5.2 is called a linear regression equation.\n\nIt describes the regression of \\(Y\\) on the independent variables \\(x_1, x_2, \\dots, x_r\\).\nThe quantities \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) are called regression coefficients, and must usually be estimated from a set of data.\n\nA regression equation can be classified based on the number of independent variables:\n\nSimple regression equation: Contains a single independent variable (\\(r = 1\\)).\nMultiple regression equation: Contains many independent variables.\n\nA simple linear regression model assumes a linear relationship between the mean response and a single independent variable.\n\nIt is expressed as: \\[\nY = \\alpha + \\beta x + e\n\\]\n\n\\(x\\) is the value of the independent variable (also called the input level).\n\\(Y\\) is the response.\n\\(e\\) represents the random error, assumed to be a random variable with mean \\(0\\).\nRegression analysis is used to find the mean value of the dependent variable \\(Y\\) given the independent variables \\(\\mathbf{x}\\).\n\nThe mean value of \\(Y\\) given \\(\\mathbf{x}\\) is denoted as \\(E[Y \\mid \\mathbf{x}]\\).\nBy estimating the regression coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\), we can predict the mean response for any given set of input variables.\nThis is particularly useful in understanding the central tendency of the response variable and making informed decisions based on the predicted mean.\n\n\n\n\n\n\n5.1.1 Least Squares Estimators Of The Regression Parameters\n\nAs mentioned previouslly, a simple linear regression model assumes a linear relationship between the mean response and a single independent variable.\n\nIt is expressed as: \\[\nY = \\alpha + \\beta x + e\n\\]\n\n\\(x\\) is the value of the independent variable (also called the input level).\n\\(Y\\) is the response.\n\\(e\\) represents the random error, assumed to be a random variable with mean \\(0\\).\n\n\nSuppose the responses \\(Y_i\\) corresponding to the input values \\(x_i\\) for \\(i = 1, \\dots, n\\) are observed and used to estimate \\(\\alpha\\) and \\(\\beta\\) in a simple linear regression model.\n\nLet \\(A\\) be the estimator of \\(\\alpha\\) and \\(B\\) be the estimator of \\(\\beta\\).\nThe estimated response for the input \\(x_i\\) is \\(A + Bx_i\\).\nThe difference between the actual response \\(Y_i\\) and the estimated response is \\((Y_i - A - Bx_i)\\) is called residual.\n\nThe sum of squares of residuals or residual sum of squares (SSR) is given by: \\[\nSSR = \\sum_{i=1}^n (Y_i - A - Bx_i)^2\n\\]\nThe method of least squares chooses \\(A\\) and \\(B\\) to minimize \\(SSR\\).\n\nTo find the minimizing values, differentiate \\(SSR\\) with respect to \\(A\\) and \\(B\\): \\[\n\\frac{\\partial SSR}{\\partial A} = -2 \\sum_{i=1}^n (Y_i - A - Bx_i)\n\\] \\[\n\\frac{\\partial SSR}{\\partial B} = -2 \\sum_{i=1}^n x_i (Y_i - A - Bx_i)\n\\]\nSetting the partial derivatives to zero yields the normal equations: \\[\n\\sum_{i=1}^n Y_i = nA + B \\sum_{i=1}^n x_i\n\\] \\[\n\\sum_{i=1}^n x_i Y_i = A \\sum_{i=1}^n x_i + B \\sum_{i=1}^n x_i^2\n\\]\n\nLet \\(\\overline{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\\) and \\(\\overline{x} = \\frac{\\sum_{i=1}^n x_i}{n}\\).\n\nThe first normal equation can be rewritten as: \\[\nA = \\overline{Y} - B \\overline{x}\n\\tag{5.3}\\]\n\nSubstituting \\(A = \\overline{Y} - B \\overline{x}\\) into the second normal equation: \\[\n\\sum_{i=1}^n x_i Y_i = (\\overline{Y} - B \\overline{x}) n \\overline{x} + B \\sum_{i=1}^n x_i^2\n\\]\nSimplifying: \\[\nB \\left( \\sum_{i=1}^n x_i^2 - n \\overline{x}^2 \\right) = \\sum_{i=1}^n x_i Y_i - n \\overline{x} \\overline{Y}\n\\]\nSolving for \\(B\\):\n\\[\nB = \\frac{\\sum_{i=1}^n x_i Y_i - n \\overline{x} \\overline{Y}}{\\sum_{i=1}^n x_i^2 - n \\overline{x}^2}\n\\]\nUsing Equation 5.3 and the fact that \\(n \\overline{x} = \\sum_{i=1}^n x_i\\), we obtain the estimators for \\(\\alpha\\) and \\(\\beta\\).\n\n\n\n\n\n\n\nLeast Squares Estimators\n\n\n\n\nThe least squares estimators of \\(\\beta\\) and \\(\\alpha\\) for the data set \\((x_i, Y_i)\\), where \\(i = 1, \\dots, n\\), are given by:\n\nThe estimator for \\(\\beta\\) (\\(B\\)): \\[\nB = \\frac{\\sum_{i=1}^n x_i Y_i - \\overline{x} \\sum_{i=1}^n Y_i}{\\sum_{i=1}^n x_i^2 - n \\overline{x}^2}\n\\]\nThe estimator for \\(\\alpha\\) (\\(A\\)): \\[\nA = \\overline{Y} - B \\overline{x}\n\\]\nHere:\n\n\\(\\overline{x} = \\frac{\\sum_{i=1}^n x_i}{n}\\) is the mean of the independent variable \\(x\\).\n\\(\\overline{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\\) is the mean of the dependent variable \\(Y\\).\n\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nThe raw material used in the production of a certain synthetic fiber is stored in a location without humidity control. Measurements of the relative humidity in the storage location and the moisture content of a sample of the raw material were taken over 15 days, resulting in the following data (in percentages):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelative Humidity (%)\n46\n53\n29\n61\n36\n39\n47\n49\n52\n38\n55\n32\n57\n54\n44\n\n\n\n\nMoisture Content (%)\n12\n15\n7\n17\n10\n11\n11\n12\n14\n9\n16\n8\n18\n14\n12\n\n\n\nUse linear regression to express moisture content as a function of the relative humidity.\n\n\n\n\nThe Coefficient of Determination and the Sample Correlation Coefficient\n\nSuppose we want to measure the amount of variation in the set of response values \\(Y_1, \\dots, Y_n\\) corresponding to the input values \\(x_1, \\dots, x_n\\).\n\nA standard measure of variation in statistics is given by: \\[\nSST = \\sum_{i=1}^n (Y_i - \\overline{Y})^2\n\\]\n\n(‘SST’ also refers to ‘Total Sum of Squares’)\n\nHere, \\(\\overline{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\\) is the mean of the response values.\nIf all \\(Y_i\\) are equal (i.e., \\(Y_i = \\overline{Y}\\) for all \\(i\\)), then \\(SST = 0\\).\n\nThe variation in the response values \\(Y_i\\) arises from two factors:\n\nDifferent input values: The input values \\(x_i\\) are different, leading to different mean values for the responses \\(Y_i\\).\nInherent variance: Even after accounting for the input values, each response \\(Y_i\\) has a variance \\(\\sigma^2\\) and will not exactly equal the predicted value at its input \\(x_i\\).\n\nTo determine how much variation is due to the different input values and how much is due to inherent variance, consider:\n\nThe quantity: \\[\nSSR = \\sum_{i=1}^n (Y_i - A - Bx_i)^2\n\\]\nmeasures the remaining variation in the response values after accounting for the input values.\nThe difference: \\[\nSST - SSR\n\\]\nrepresents the variation in the response values explained by the different input values.\n\nThe coefficient of determination (\\(R^2\\)) is defined as the proportion of the total variation in the response values that is explained by the input values: \\[\nR^2 = \\frac{SST - SSR}{SST}\n\\]\n\n\\(R^2\\) ranges from 0 to 1.\nAn \\(R^2\\) value of 1 indicates that the regression model perfectly explains the variation in the response values.\nAn \\(R^2\\) value of 0 indicates that the regression model does not explain any of the variation in the response values.\n\nThe sample correlation coefficient (\\(r\\)) measures the strength and direction of the linear relationship between the independent variable \\(x\\) and the dependent variable \\(Y\\): \\[\nr = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2}} = \\sqrt{R^2}\n\\]\n\nNote that the sign of r is the sign of B.\n\\(r\\) ranges from -1 to 1.\nAn \\(r\\) value of 1 indicates a perfect positive linear relationship.\nAn \\(r\\) value of -1 indicates a perfect negative linear relationship.\nAn \\(r\\) value of 0 indicates no linear relationship.\n\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nFind the coefficient of determination and the sample correlation coefficient of the previous regression problem.\n\n\n\n    import numpy as np\n    from sklearn.linear_model import LinearRegression\n    from sklearn.metrics import r2_score\n    import matplotlib.pyplot as plt\n\n    # Data for relative humidity and moisture content\n    humidity = np.array([46, 53, 29, 61, 36, 39, 47, 49, 52, 38, 55, 32, 57, 54, 44]).reshape(-1, 1)\n    moisture = np.array([12, 15, 7, 17, 10, 11, 11, 12, 14, 9, 16, 8, 18, 14, 12])\n\n    # Fit the linear regression model\n    model = LinearRegression().fit(humidity, moisture)\n\n    # Print the coefficients\n    print(f\"Intercept: {model.intercept_}\")\n    print(f\"Coefficient: {model.coef_[0]}\")\n\n    # Predict the moisture content\n    moisture_pred = model.predict(humidity)\n\n    # Calculate the coefficient of determination (R^2)\n    R_squared = r2_score(moisture, moisture_pred)\n    print(f\"Coefficient of Determination (R^2): {R_squared}\")\n\n    # Calculate the sample correlation coefficient (r)\n    correlation_coefficient = np.sqrt(R_squared)\n    print(f\"Sample Correlation Coefficient (r): {correlation_coefficient}\")\n\n    # Plot the data points\n    plt.scatter(humidity, moisture, color='blue', label='Data Points')\n\n    # Plot the regression line\n    plt.plot(humidity, moisture_pred, color='red', label='Regression Line')\n\n    # Add labels and title\n    plt.xlabel('Relative Humidity (%)')\n    plt.ylabel('Moisture Content (%)')\n    plt.title('Linear Regression: Moisture Content vs. Relative Humidity')\n    plt.legend()\n\n    # Show the plot\n    plt.grid(True)\n    plt.show()\n\nIntercept: -2.5104576516877177\nCoefficient: 0.32320356181404014\nCoefficient of Determination (R^2): 0.9113639730826797\nSample Correlation Coefficient (r): 0.9546538498757965\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Multiple Linear Regression\n\nOverview:\n\nIn most applications, the response \\(Y\\) of an experiment is better predicted using multiple independent input variables rather than a single one.\nA typical scenario involves \\(k\\) input variables, and the response \\(Y\\) is related to them by the equation: \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + e\n\\]\nwhere:\n\n\\(x_j\\) is the level of the \\(j^{th}\\) input variable (\\(j = 1, \\dots, k\\)),\n\\(e\\) is a random error term, assumed to be normally distributed with mean \\(0\\) and constant variance \\(\\sigma^2\\).\n\n\nParameters:\n\nThe parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) and \\(\\sigma^2\\) are unknown and must be estimated from the data.\nThe data consists of observed values \\(Y_1, Y_2, \\dots, Y_n\\), where each \\(Y_i\\) corresponds to a set of input levels \\(x_{i1}, x_{i2}, \\dots, x_{ik}\\).\n\nExpected Value:\n\nThe expected value of \\(Y_i\\) is given by: \\[\nE[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik}\n\\]\n\n\n\nDetermining the Least Squares Estimators\n\nLeast Squares Estimation:\n\nLet \\(B_0, B_1, \\dots, B_k\\) denote estimators of \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\).\nThe sum of squared differences between the observed \\(Y_i\\) and their estimated expected values is: \\[\n\\sum_{i=1}^n (Y_i - B_0 - B_1 x_{i1} - B_2 x_{i2} - \\cdots - B_k x_{ik})^2\n\\]\nThe least squares estimators are the values of \\(B_0, B_1, \\dots, B_k\\) that minimize the above sum of squared differences.\n\nMethod:\n\nTo determine the least squares estimators \\(B_0, B_1, \\dots, B_k\\), we take partial derivatives of the sum of squared differences with respect to each estimator:\n\nFirst, with respect to \\(B_0\\),\nThen, with respect to \\(B_1\\),\nAnd so on, up to \\(B_k\\).\n\nBy setting these partial derivatives equal to \\(0\\), we obtain a system of \\(k + 1\\) equations.\n\nSystem of Equations:\n\nThe partial derivatives yield the following equations: \\[\n\\sum_{i=1}^n (Y_i - B_0 - B_1 x_{i1} - B_2 x_{i2} - \\cdots - B_k x_{ik}) = 0\n\\] \\[\n\\sum_{i=1}^n x_{i1} (Y_i - B_0 - B_1 x_{i1} - \\cdots - B_k x_{ik}) = 0\n\\] \\[\n\\sum_{i=1}^n x_{i2} (Y_i - B_0 - B_1 x_{i1} - \\cdots - B_k x_{ik}) = 0\n\\] \\[\n\\vdots\n\\] \\[\n\\sum_{i=1}^n x_{ik} (Y_i - B_0 - B_1 x_{i1} - \\cdots - B_k x_{ik}) = 0\n\\]\n\nNormal Equations:\n\nRewriting the above equations, the least squares estimators \\(B_0, B_1, \\dots, B_k\\) satisfy the following normal equations: \\[\n\\sum_{i=1}^n Y_i = n B_0 + B_1 \\sum_{i=1}^n x_{i1} + B_2 \\sum_{i=1}^n x_{i2} + \\cdots + B_k \\sum_{i=1}^n x_{ik}\n\\] \\[\n\\sum_{i=1}^n x_{i1} Y_i = B_0 \\sum_{i=1}^n x_{i1} + B_1 \\sum_{i=1}^n x_{i1}^2 + B_2 \\sum_{i=1}^n x_{i1} x_{i2} + \\cdots + B_k \\sum_{i=1}^n x_{i1} x_{ik}\n\\] \\[\n\\vdots\n\\] \\[\n\\sum_{i=1}^n x_{ik} Y_i = B_0 \\sum_{i=1}^n x_{ik} + B_1 \\sum_{i=1}^n x_{ik} x_{i1} + B_2 \\sum_{i=1}^n x_{ik} x_{i2} + \\cdots + B_k \\sum_{i=1}^n x_{ik}^2\n\\]\nThese equations form a linear system that can be solved to find the least squares estimators \\(B_0, B_1, \\dots, B_k\\).\n\n5.1.3 Normal Equations in Matrix Notation\nNormal Equations:\n\nThe normal equations for the least squares estimators \\(B_0, B_1, \\dots, B_k\\) are given by: \\[\n\\sum_{i=1}^n x_{i1} Y_i = B_0 \\sum_{i=1}^n x_{i1} + B_1 \\sum_{i=1}^n x_{i1}^2 + B_2 \\sum_{i=1}^n x_{i1} x_{i2} + \\cdots + B_k \\sum_{i=1}^n x_{i1} x_{ik}\n\\] \\[\n\\vdots\n\\] \\[\n\\sum_{i=1}^n x_{ik} Y_i = B_0 \\sum_{i=1}^n x_{ik} + B_1 \\sum_{i=1}^n x_{ik} x_{i1} + B_2 \\sum_{i=1}^n x_{ik} x_{i2} + \\cdots + B_k \\sum_{i=1}^n x_{ik}^2\n\\]\n\nMatrix Notation:\n\nTo simplify the solution of the normal equations, we introduce matrix notation.\nLet:\n\n\\(Y\\) be the response vector: \\[\nY = \\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_n\n\\end{bmatrix}\n\\]\n\\(X\\) be the design matrix: \\[\nX = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\n\\]\n\n5.1.4 Matrix Representation of Multiple Regression\n\n\\(\\beta\\) be the parameter vector: \\[\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}\n\\]\n\\(e\\) be the error vector: \\[\ne = \\begin{bmatrix}\ne_1 \\\\\ne_2 \\\\\n\\vdots \\\\\ne_n\n\\end{bmatrix}\n\\]\n\n\nDimensions:\n\n\\(Y\\) is an \\(n \\times 1\\) matrix (response vector),\n\\(X\\) is an \\(n \\times p\\) matrix (design matrix), where \\(p = k + 1\\),\n\\(\\beta\\) is a \\(p \\times 1\\) matrix (parameter vector),\n\\(e\\) is an \\(n \\times 1\\) matrix (error vector).\n\nMultiple Regression Model:\n\nThe multiple regression model can be written in matrix form as: \\[\nY = X \\beta + e\n\\]\n\nLeast Squares Estimators:\n\nLet \\(B\\) be the matrix of least squares estimators: \\[\nB = \\begin{bmatrix}\nB_0 \\\\\nB_1 \\\\\n\\vdots \\\\\nB_k\n\\end{bmatrix}\n\\]\nThe normal equations can be written in matrix form as: \\[\nX^T Y = X^T X B  \n\\] where:\n\n\\(X^T\\) is the transpose of the design matrix \\(X\\),\n\\(X^T X\\) is a \\(p \\times p\\) matrix,\n\\(X^T Y\\) is a \\(p \\times 1\\) vector.\n\n\nSolution:\n\nThe least squares estimators \\(B\\) can be obtained by solving the matrix equation: \\[\nB = (X^T X)^{-1} X^T Y\n\\] where \\((X^T X)^{-1}\\) is the inverse of the matrix \\(X^T X\\).\n\n\n\n\n\nResiduals and Sum of Squared Residuals (SSR)\n\nResiduals:\n\nLet \\(r_i\\) denote the \\(i^{th}\\) residual, which is the difference between the observed response \\(Y_i\\) and the predicted value from the regression model: \\[\nr_i = Y_i - B_0 - B_1 x_{i1} - B_2 x_{i2} - \\cdots - B_k x_{ik}, \\quad i = 1, \\dots, n\n\\]\nThe residual vector \\(r\\) is defined as: \\[\nr = \\begin{bmatrix}\nr_1 \\\\\nr_2 \\\\\n\\vdots \\\\\nr_n\n\\end{bmatrix}\n\\]\nIn matrix notation, the residual vector can be expressed as: \\[\nr = Y - XB\n\\] where:\n\n\\(Y\\) is the response vector,\n\\(X\\) is the design matrix,\n\\(B\\) is the matrix of least squares estimators.\n\n\nSum of Squared Residuals (SSR):\n\nThe sum of squared residuals (SSR) is given by: \\[\nSSR = \\sum_{i=1}^n r_i^2\n\\]\nIn matrix notation, the SSR can be written as: \\[\nSSR = r^T r\n\\] where \\(r^T\\) is the transpose of the residual vector \\(r\\).\nSubstituting \\(r = Y - XB\\), we get: \\[\nSSR = (Y - XB)^T (Y - XB)\n\\]\nExpanding the matrix product: \\[\nSSR = [Y^T - (XB)^T](Y - XB)\n\\]\n\\[\nSSR = Y^T Y - Y^T XB - B^T X^T Y + B^T X^T XB\n\\]\n\nSimplification Using Normal Equations:\n\nFrom the normal equations, we know: \\[\nX^T XB = X^T Y\n\\]\nSubstituting this into the expression for SSR: \\[\nSSR = Y^T Y - Y^T XB - B^T X^T Y + B^T X^T XB\n\\] \\[\nSSR = Y^T Y - Y^T XB\n\\] (since \\(B^T X^T Y = B^T X^T XB\\) from the normal equations).\n\nTranspose Property:\n\nSince \\(Y^T XB\\) is a scalar (a \\(1 \\times 1\\) matrix), it is equal to its transpose: \\[\nY^T XB = (Y^T XB)^T = B^T X^T Y\n\\]\nTherefore, the expression for SSR simplifies further to: \\[\nSSR = Y^T Y - B^T X^T Y\n\\]\n\nFinal Computational Formula for SSR:\n\nThe sum of squared residuals (SSR) can be computed using the formula: \\[\nSSR = Y^T Y - B^T X^T Y\n\\]\nThis formula is computationally efficient but requires careful handling to avoid roundoff errors.\n\nInterpretation:\n\nThe term \\(Y^T Y\\) represents the total variation in the response variable \\(Y\\).\nThe term \\(B^T X^T Y\\) represents the explained variation by the regression model.\nThe difference \\(Y^T Y - B^T X^T Y\\) gives the unexplained variation (SSR), which is minimized in the least squares method.\n\n\n\n\nCoefficient of Multiple Determination (\\(R^2\\))\n\nDefinition:\n\nThe coefficient of multiple determination, denoted by \\(R^2\\), measures the proportion of the total variation in the response variable \\(Y\\) that is explained by the regression model.\nIt is defined as: \\[\nR^2 = 1 - \\frac{SSR}{\\sum_{i=1}^n (Y_i - \\overline{Y})^2}\n\\] where:\n\n\\(SSR\\) is the sum of squared residuals (unexplained variation),\n\\(\\sum_{i=1}^n (Y_i - \\overline{Y})^2\\) is the total sum of squares (SST) (total variation in \\(Y\\)),\n\\(\\overline{Y}\\) is the mean of the observed response values \\(Y_i\\).\n\n\nRange of \\(R^2\\):\n\n\\(R^2\\) ranges between \\(0\\) and \\(1\\):\n\n\\(R^2 = 0\\): The regression model explains none of the variation in \\(Y\\).\n\\(R^2 = 1\\): The regression model explains all of the variation in \\(Y\\).\n\n\nFormula in Terms of SST and SSR:\n\nThe formula for \\(R^2\\) can also be written as: \\[\nR^2 = \\frac{SST - SSR}{SST}\n\\]\nwhere:\n\n\\(SST = \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\) is the total sum of squares,\n\\(SSR = \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) is the sum of squared residuals.\n\n\nUsefulness:\n\n\\(R^2\\) is a useful measure of the goodness of fit of the regression model.\nA higher \\(R^2\\) indicates that the model explains a larger proportion of the variation in the response variable \\(Y\\).\n\n\n### Probabilistic Perspective of Linear Regression\n\nOverview:\n\nLinear regression can also be viewed from a probabilistic perspective.\nIn this view, the response variable \\(Y\\) is considered a random variable with a probability distribution that depends on the input variables \\(x_1, x_2, \\dots, x_r\\).\n\nAssumptions:\n\nThe response variable \\(Y\\) is normally distributed with mean \\(\\mu_Y\\) and variance \\(\\sigma^2\\).\nThe mean \\(\\mu_Y\\) is a linear function of the input variables: \\[\n\\mu_Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r\n\\]\nThe variance \\(\\sigma^2\\) is constant and does not depend on the input variables.\n\nModel:\n\nThe linear regression model can be written as: \\[\nY = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r + \\epsilon\n\\] where \\(\\epsilon\\) is a random error term that follows a normal distribution with mean \\(0\\) and variance \\(\\sigma^2\\): \\[\n\\epsilon \\sim N(0, \\sigma^2)\n\\]\n\nLikelihood Function:\n\nGiven a set of observed data points \\((x_i, Y_i)\\) for \\(i = 1, \\dots, n\\), the likelihood function represents the probability of observing the data given the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) and \\(\\sigma^2\\).\nThe likelihood function is given by: \\[\nL(\\beta_0, \\beta_1, \\dots, \\beta_r, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(Y_i - \\beta_0 - \\beta_1 x_{i1} - \\dots - \\beta_r x_{ir})^2}{2 \\sigma^2} \\right)\n\\]\n\nLog-Likelihood Function:\n\nThe log-likelihood function is the natural logarithm of the likelihood function: \\[\n\\log L(\\beta_0, \\beta_1, \\dots, \\beta_r, \\sigma^2) = -\\frac{n}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (Y_i - \\beta_0 - \\beta_1 x_{i1} - \\dots - \\beta_r x_{ir})^2\n\\]\n\nMaximum Likelihood Estimation (MLE):\n\nThe parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) and \\(\\sigma^2\\) can be estimated by maximizing the log-likelihood function.\nThe maximum likelihood estimators (MLEs) of \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) are the same as the least squares estimators.\nThe MLE of \\(\\sigma^2\\) is given by: \\[\n\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\n\\] where \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\dots + \\hat{\\beta}_r x_{ir}\\) are the predicted values from the regression model.\n\nInference:\n\nThe probabilistic perspective allows for statistical inference about the regression parameters.\nConfidence intervals and hypothesis tests can be constructed for the parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) and \\(\\sigma^2\\).\n\n\n\n    import numpy as np\n    from scipy.stats import norm\n\n    # Generate synthetic data\n    np.random.seed(42)\n    n = 100\n    x = np.linspace(0, 10, n)\n    beta_0 = 2.0\n    beta_1 = 1.5\n    sigma = 1.0\n    epsilon = np.random.normal(0, sigma, n)\n    Y = beta_0 + beta_1 * x + epsilon\n\n    # Maximum Likelihood Estimation (MLE) for beta_0, beta_1, and sigma^2\n    X = np.vstack([np.ones(n), x]).T\n\n    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ Y\n    \n    Y_hat = X @ beta_hat\n    sigma_hat = np.sqrt(np.sum((Y - Y_hat) ** 2) / n)\n\n    print(f\"Estimated beta_0: {beta_hat[0]}\")\n    print(f\"Estimated beta_1: {beta_hat[1]}\")\n    print(f\"Estimated sigma^2: {sigma_hat ** 2}\")\n\n    # Plot the data and the fitted line\n    import matplotlib.pyplot as plt\n\n    plt.scatter(x, Y, color='blue', label='Data Points')\n    plt.plot(x, Y_hat, color='red', label='Fitted Line')\n    plt.xlabel('Independent Variable (x)')\n    plt.ylabel('Dependent Variable (Y)')\n    plt.title('Probabilistic Linear Regression')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nEstimated beta_0: 1.8271871459226277\nEstimated beta_1: 1.5137932673366563\nEstimated sigma^2: 0.8149047134980785",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#logistic-regression",
    "href": "Regression.html#logistic-regression",
    "title": "5  Regression",
    "section": "5.2 Logistic Regression",
    "text": "5.2 Logistic Regression\n\nOverview:\n\nLogistic regression is used when the response variable is categorical, typically binary (0 or 1).\nIt models the probability that a given input point belongs to a particular category.\n\nLogistic Function:\n\nThe logistic function (also called the sigmoid function) is used to model the probability: \\[\nP(Y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r))}\n\\]\n\nLogistic Regression Model:\n\nThe logistic regression model can be written as: \\[\n\\log \\left( \\frac{P(Y = 1 \\mid \\mathbf{x})}{1 - P(Y = 1 \\mid \\mathbf{x})} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_r x_r\n\\]\nHere, the left-hand side is the log-odds (logit) of the probability of the response being 1.\n\nEstimation of Parameters:\n\nThe parameters \\(\\beta_0, \\beta_1, \\dots, \\beta_r\\) are estimated using the method of maximum likelihood.\nThe likelihood function for logistic regression is given by: \\[\nL(\\beta_0, \\beta_1, \\dots, \\beta_r) = \\prod_{i=1}^n P(Y_i \\mid \\mathbf{x}_i)^{Y_i} (1 - P(Y_i \\mid \\mathbf{x}_i))^{1 - Y_i}\n\\]\n\nLog-Likelihood Function:\n\nThe log-likelihood function is: \\[\n\\log L(\\beta_0, \\beta_1, \\dots, \\beta_r) = \\sum_{i=1}^n \\left[ Y_i \\log P(Y_i \\mid \\mathbf{x}_i) + (1 - Y_i) \\log (1 - P(Y_i \\mid \\mathbf{x}_i)) \\right]\n\\]\n\nFitting the Model:\n\nThe parameters are estimated by maximizing the log-likelihood function using numerical optimization techniques.\n\n\n\nChecking the goodness of fit\n\n\n5.2.1 Confusion Matrix, Accuracy, and ROC Curve\n\nConfusion Matrix:\n\nA confusion matrix is a table used to evaluate the performance of a classification model.\nIt summarizes the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.\nThe matrix helps in understanding the types of errors the model is making.\n\nAccuracy:\n\nAccuracy is a metric that measures the proportion of correct predictions made by the model.\nIt is calculated as the sum of true positives and true negatives divided by the total number of predictions: \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nWhile accuracy is a useful metric, it may not be sufficient for imbalanced datasets where one class is more frequent than the other.\n\n6 Calculate sensitivity (recall) and specificity\nsensitivity = TP / (TP + FN) specificity = TN / (TN + FP)\nprint(f”Sensitivity (Recall): {sensitivity}“) print(f”Specificity: {specificity}“)\nROC Curve:\n\nThe Receiver Operating Characteristic (ROC) curve is a graphical representation of a classification model’s performance.\nIt plots the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\nThe area under the ROC curve (AUC) is a single scalar value that summarizes the model’s ability to discriminate between positive and negative classes.\nAn AUC value of 1 indicates perfect classification, while an AUC value of 0.5 indicates no discriminative power (equivalent to random guessing).\n\n\n\n  import numpy as np\n  from sklearn.linear_model import LogisticRegression\n  from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n  import matplotlib.pyplot as plt\n\n  # Generate synthetic data for logistic regression\n  np.random.seed(42)\n  n = 100\n  x = np.linspace(0, 10, n)\n  beta_0 = -5.0\n  beta_1 = 1.0\n  prob = 1 / (1 + np.exp(-(beta_0 + beta_1 * x)))\n  Y = np.random.binomial(1, prob, n)\n\n  # Reshape x for sklearn\n  x = x.reshape(-1, 1)\n\n  # Fit the logistic regression model\n  model = LogisticRegression().fit(x, Y)\n\n  # Predict probabilities\n  Y_prob = model.predict_proba(x)[:, 1]\n\n  # Predict class labels\n  Y_pred = model.predict(x)\n\n  # Calculate confusion matrix\n  conf_matrix = confusion_matrix(Y, Y_pred)\n  print(\"Confusion Matrix:\")\n  print(conf_matrix)\n\n  # Calculate accuracy\n  accuracy = accuracy_score(Y, Y_pred)\n  print(f\"Accuracy: {accuracy}\")\n\n  # Calculate ROC curve and AUC\n  fpr, tpr, _ = roc_curve(Y, Y_prob)\n  roc_auc = auc(fpr, tpr)\n  print(f\"AUC: {roc_auc}\")\n\n  # Plot ROC curve\n  plt.figure()\n  plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel('False Positive Rate')\n  plt.ylabel('True Positive Rate')\n  plt.title('Receiver Operating Characteristic (ROC) Curve')\n  plt.legend(loc=\"lower right\")\n  plt.grid(True)\n  plt.show()\n\nConfusion Matrix:\n[[47  5]\n [ 5 43]]\nAccuracy: 0.9\nAUC: 0.9647435897435896",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression</span>"
    ]
  }
]