[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling - 24DS636 (2024-25)",
    "section": "",
    "text": "Course Introduction\nThe website contains course contents of “Statistical Modeling” offered by Abhijith M S, PhD to Masters students pursuing M.Tech in Data Science, during the even semester of the academic year 2024-25.\n\n\nSyllabus\n(As given in the curriculum)\n\nProbability, Random Variables & Probability Distributions.\nSampling, analysis of sample data-Empirical Distributions, Sampling from a Population Estimation, confidence intervals, point estimation–Maximum Likelihood, Probability mass functions, Modeling distributions, Hypothesis testing- Z, t, Chi-Square.\nANOVA & Designs of Experiments - Single, Two factor ANOVA, Factorials ANOVA models.\nLinear least squares, Correlation & Regression Models-linear regression methods, Ridge regression, LASSO, univariate and Multivariate Linear Regression, probabilistic interpretation, Regularization, Logistic regression, locally weighted regression.\nExploratory data analysis, Time series analysis, Analytical methods – ARIMA and SARIMA.\n\n\n\nEvaluations: A Tentative Timeline\n\nBest two marks out of three quizzes (Total = 20 marks)\nQuiz-1 (10 marks): (January First week)\nQuiz-2 (10 marks):(March First week)\nQuiz-3 (10 marks):(April First week)\nAssignments (Total = 30 marks)\nAssignment-1 (10 marks):(Submission: End of January)\nAssignment-2 (10 marks):(Submission: End of March)\nProject Review - 1 (10 marks):(February second week)\nMid Sem (Total = 20 marks)\nMid-Semester Exam (20 marks):(Feb first week, as per Academic calender)\nEnd Sem (Total = 30 marks)\nEnd-Semester Project Presentation (20 marks):(April second week, as per Academic calender) \\end{itemize}$\n\nContact: ms_abhijith@cb.amrita.edu",
    "crumbs": [
      "Course Introduction"
    ]
  },
  {
    "objectID": "DescriptiveStatistics.html",
    "href": "DescriptiveStatistics.html",
    "title": "1  Descriptive statistics",
    "section": "",
    "text": "Descriptive statistics deals with methods to describe and summarize data.\nDescribing of data is effectively done through tables or graphs. Those often reveal important features such as the range, the degree of concentration, and the symmetry of the data.\nThe summary of data is expressed through numerical quantities (summary statistics) whose values are determined by the data.\n\n\n1.0.1 Describing Data sets\n\n\n1.0.2 Frequency Tables and Graphs\n\nA data set having a relatively small number of distinct values can be conveniently presented in a frequency table.\nData from a frequency table can be graphically represented by:\n\nLine Graph\nBar Graph\nFrequency Polygon\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\n# Line Graph\nplt.plot(values, frequencies, marker='o')\nplt.title('Frequency Polygon')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\nplt.bar(values, frequencies, width=0.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.3 Relative Frequency Tables and Graphs\n\nConsider a data set consisting of n values. If f is the frequency of a particular value, then the ratio f /n is called its relative frequency.\nThat is, the relative frequency of a data value is the proportion of the data that have that value.\nThe relative frequencies can be represented graphically by:\n\nrelative frequency line\nrelative frequency bar graph\nrelative frequency polygon\npie chart: A pie chart is often used to indicate relative frequencies when the data are not numerical in nature. A circle is constructed and then sliced into different sectors with areas proportional to the respective relative frequencies.\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\nrelative_frequencies = frequencies/len(data)\nprint(relative_frequencies)\n\n\nplt.pie(relative_frequencies, labels = values, autopct='%1.1f%%')\nplt.show()\n\n[0.2        0.46666667 0.33333333]\n\n\n\n\n\n\n\n\n\n\n\n1.0.4 Grouped Data, Histograms, Ogives, and Stem and Leaf Plots\n\nFor some data sets the number of distinct values is too large to utilize frequency tables.\nInstead, in such cases, it is useful to divide the values into groupings, or class intervals, and then plot the number of data values falling in each class interval.\nThe number of class intervals chosen should be a trade-off between:\n\nchoosing too few classes at a cost of losing too much information about the actual data values in a class.\nchoosing too many classes, which will result in the frequencies of each class being too small.\n\nIt is common, although not essential, to choose class intervals of equal length.\nThe endpoints of a class interval are called the class boundaries.\nWe will adopt the left-end inclusion convention, which stipulates that a class interval contains its left-end but not its right-end boundary point.\nThus, for instance, the class interval 20-30 contains all values that are both greater than or equal to 20 and less than 30.\nA bar graph plot of class data, with the bars placed adjacent to each other, is called a histogram.\nThe vertical axis of a histogram can represent either the class frequency or the relative class frequency.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\n# Plot histogram\nplt.hist(random_integers, bins=bins, edgecolor='black')\nplt.title('Histogram of Random Integers')\nplt.xlabel('Class Interval')\nplt.ylabel('Frequency')\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n\n\n\n\n\n\n\n\n\n\nWe are sometimes interested in plotting a cumulative frequency (or cumulative relative frequency) graph.\nA point on the horizontal axis of such a graph represents a possible data value; its corresponding vertical plot gives the number (or proportion) of the data whose values are less than or equal to it.\nA cumulative frequency plot is called an ogive.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\nhistograms = np.histogram(random_integers, bins=bins)[0]\nprint(histograms)\ncumulativeSum = np.cumsum(histograms) \nprint(cumulativeSum)\n\n\nplt.plot(bins[:-1], cumulativeSum, marker='o', linestyle='-')\nplt.title('Ogive (Cumulative Frequency Graph)')\nplt.xlabel('Class Interval')\nplt.ylabel('Cumulative Frequency')\n#plt.grid(True)\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n[22 25 15 24 14 21 20 24 19 21 23 21 28 23]\n[ 22  47  62  86 100 121 141 165 184 205 228 249 277 300]\n\n\n\n\n\n\n\n\n\n\nAn efficient way of organizing a small-to moderate-sized data set is to utilize a stem and leaf plot.\nSuch a plot is obtained by first dividing each data value into two parts - its stem and its leaf.\nFor instance, if the data are all two-digit numbers, then we could let the stem part of a data value be its tens digit and let the leaf be its ones digit.\nThus, for instance, the value 62 is expressed as\n\n\nimport numpy as np\n\n# Sample data\ndata = np.array([62, 67, 63, 68, 69, 61, 64, 65, 66, 60, 75, 74, 76, 78, 90, 92, 34, 36, 56, 57, 45, 53, 52, 59, 73, 74, 79, 80, 81,20, 34, 36, 85])\n\n# Create stem and leaf plot\nstem_leaf = {}\n\nfor number in data:\n    stem = number // 10\n    leaf = number % 10\n    if stem in stem_leaf:\n        stem_leaf[stem].append(leaf)\n    else:\n        stem_leaf[stem] = [leaf]\n\n# Print stem and leaf plot\nfor stem, leaves in sorted(stem_leaf.items()):\n    print(f\"{stem} | {' '.join(map(str, sorted(leaves)))}\")\n\n2 | 0\n3 | 4 4 6 6\n4 | 5\n5 | 2 3 6 7 9\n6 | 0 1 2 3 4 5 6 7 8 9\n7 | 3 4 4 5 6 8 9\n8 | 0 1 5\n9 | 0 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html",
    "href": "ParameterEstimation.html",
    "title": "2  Parameter Estimation",
    "section": "",
    "text": "2.1 Point Estimation: Maximum Likelihood Estimators\n(Please refer the book titled “Introduction to Probability and Statistics for Engineers and Scientists” by Sheldon M Ross for more details)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html#interval-estimates",
    "href": "ParameterEstimation.html#interval-estimates",
    "title": "2  Parameter Estimation",
    "section": "2.2 Interval Estimates",
    "text": "2.2 Interval Estimates\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a known distribution with an unknown mean \\(\\mu\\).\nIt is established that the sample mean \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) serves as the maximum likelihood estimator for \\(\\mu\\).\nHowever, the sample mean \\(\\bar{X}\\) is not expected to be exactly equal to \\(\\mu\\), but rather close to it.\nTherefore, instead of providing a single point estimate, it is often more useful to specify an interval within which we are confident that \\(\\mu\\) lies.\nTo determine such an interval estimator, we utilize the probability distribution of the point estimator.\n\n\n2.2.1 Confidence Intervals for the Mean of a normal population with known Variance\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nThe point estimator \\(\\bar{X}\\) is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\)/n.\nTherefore, \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\) follows a standard normal distribution.\n\n\n\n\n\n\n\nWhat to do\n\n\n\nConsider that I want to find an interval around \\(\\bar{X}\\) such that the actual population mean \\(\\mu\\) falls within the interval, say 95 % of the times.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor finding such an interval, I can use the Z-table. From the Z-table I can find:\n\n\\[\nP\\left( -1.96 &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96 \\right) = 0.9750 - 0.0250 = 0.95\n\\]\n\nRewriting the above equation:\n\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} -\\mu &lt; 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( 1.96\\frac{\\sigma}{\\sqrt{n}} &gt; \\mu - \\bar{X} &gt; -1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu - \\bar{X} &lt; 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\nWe have P(Z &lt; -1.96) = 0.025, similarly P(Z &gt; 1.96) = 0.025. Usually 1.96 is represented generally as \\(z_{0.025}\\). Thus, P(Z &lt; -z\\(_{0.025}\\)) = 0.025 and P(Z &gt; z\\(_{0.025}\\)) = 0.025.\nHence, 100(1-0.05) percent confidence interval for the mean of a normal population with known variance is:\n\n\\[\nP\\left( \\bar{X} - z_{0.025}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}}\\right) = (1 - 0.05)\n\\]\n\n\n\nFor a confidence level of \\(100(1-\\alpha)\\) percent, the corresponding critical value from the standard normal distribution is \\(z_{\\alpha/2}\\).\nThe \\(100(1-\\alpha)\\) percent confidence interval for \\(\\mu\\) is given by:\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.1}\\]\n\nThe interval as given in Equation 2.1 is called a two-sided confidence interval.\nAlso the term \\(z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\) is called the margin of error.\n\n\n\n\n\n\n\nDerivation of two-sided confidence interval\n\n\n\n\nTo find 100(1-\\(\\alpha\\)) percent confidence interval of mean (\\(\\mu\\)), we have;\n\n\\[\nP\\left( -z_{\\alpha/2} &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; z{_\\alpha/2} \\right) = 1 - \\alpha\n\\]\n\nDoing the same manipulations we did earlier for obtaining the 95percent confidence interval we can obtain:\n\n\\[\nP\\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nThe above equation give us the required confidence interval, as given in Equation 2.1.\n\n\n\n\n\n\n\n\n\nWhat if !?\n\n\n\nWhat if we are interested in one sided confidence intervals !!?\n\n\n\n\n\n\n\n\nOne-sided Upper Confidence Iterval\n\n\n\n\nTo determine such an interval, for a standard normal random variable Z, we have;\n\n\\[\nP\\left( Z &lt; 1.645 \\right) = 0.95\n\\]\n\nThus, \\[\nP\\left( \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} &lt; 1.645 \\right) = 0.95\n\\]\n\n\\[\nP\\left( \\mu -\\bar{X} &gt; - 1.645\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}  \\right) = 0.95\n\\]\n\nThus a 95 percent one-sided upper confidence interval for \\(\\mu\\) is\n\n\\[\n\\mu \\in \\left( \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\nor in other words; 100(1-0.05) percent one-sided upper confidence interval for \\(\\mu\\) is\n\\[\n\\mu \\in \\left( \\bar{X} - z_{0.05}\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\n\n\n\n\n\n\n\n\nOneside interval!\n\n\n\nCan you think of another one sided confidence interval?\n\n\n\n\n\n\n\n\nOne-sided lower confidence interval\n\n\n\n\nWe have \\[\nP\\left( Z &gt; - 1.645 \\right) = 0.95\n\\]\nProceed just like in the previous case and you will find a 100(1-0.05) percent one-sided lower confidence interval for \\(\\mu\\) as;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{0.05}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\]\n\n\n\nIn general, 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval for \\(\\mu\\) is given in Equation 2.2.\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}, \\infty \\right)\n\\tag{2.2}\\]\n\nAlso, 100(1-\\(\\alpha\\))percent one-sided lower confidence interval for \\(\\mu\\) is given in Equation 2.3. \\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.3}\\]\nThe python code below creates a sample and find 95% confidence interval for the mean if the population standard deviation is assumed to be 10. Other values are specified in the code.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 50  # true mean\nsigma = 10  # known standard deviation\nn = 30  # sample size\nalpha = 0.05  # significance level\n\n# Generate a sample\nnp.random.seed(0)\nsample = np.random.normal(mu, sigma, n)\nsample_mean = np.mean(sample)\n\n# Calculate the confidence interval\nz = 1.96  # z-value for 95% confidence\nmargin_of_error = z * (sigma / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\n# Plot the sample and confidence interval\nplt.figure(figsize=(8, 4))\nplt.hist(sample, bins=10, alpha=0.7, color='blue', edgecolor='black')\nplt.axvline(sample_mean, color='red', linestyle='dashed', linewidth=2, label='Sample Mean')\nplt.axvline(confidence_interval[0], color='green', linestyle='dashed', linewidth=2, label='95% CI Lower Bound')\nplt.axvline(confidence_interval[1], color='green', linestyle='dashed', linewidth=2, label='95% CI Upper Bound')\nplt.title('Sample Distribution with 95% Confidence Interval')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"95% Confidence Interval: {confidence_interval}\")\n\n\n\n\n\n\n\n\nSample Mean: 54.42856447263174\n95% Confidence Interval: (50.85011043026466, 58.007018514998826)\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose that when a signal having value \\(\\mu\\) is transmitted from location A the value received at location B is normally distributed with mean \\(\\mu\\) and variance 4. That is, if \\(\\mu\\) is sent, then the value received is \\(\\mu\\) + N where N, representing noise, is normal with mean 0 and variance 4. To reduce error, suppose the same value is sent 9 times. If the successive values received are 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5;\n(a). construct a 95 percent two-sided confidence interval for \\(\\mu\\).\n(b). construct 95 percent one-sided upper and lower confidence intervals for \\(\\mu\\).\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose a quality control manager at a factory wants to ensure that the average weight of a product is at least 500 grams. They take a random sample of 30 products and find the sample mean weight to be 495 grams with a standard deviation of 10 grams. Help the manager to estimate the minimum average weight of the products with 95% confidence.\n\n\n\n\n2.2.2 Confidence Intervals for the Mean of a normal population with unknown Variance\n\nIf you recollect the discussion we had about the sample mean from a normal population with unknown variance we saw that variable t\\(_{n-1}\\) given by:\n\n\\[\nt_{n-1} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{S}\n\\]\nhas a t-distribution with n-1 degrees of freedom.\n\nBecause of the symmetry of the t-distribution we can write for any \\(\\alpha\\) \\(\\in\\) (0, 1/2);\n\n\\[\nP\\left( -t_{\\alpha/2, n-1} &lt; \\sqrt{n}\\frac{\\bar{X} - \\mu}{S} &lt;  t_{\\alpha/2, n-1} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( -\\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &lt;  - \\mu &lt; -\\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &gt;   \\mu &gt; \\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nIf the sample mean is \\(\\bar{X}\\) and sample standard deviation S, then we can say that with 100(1-\\(\\alpha\\)) percent confidence that\n\n\\[\n\\mu \\in \\left(\\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}},  \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} \\right)\n\\]\n\nIn this case 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval can be obtained from the fact that:\n\n\\[\nP\\left( \\sqrt{n}\\frac{(\\bar{X} - \\mu)}{S} &lt; t_{\\alpha, n-1}\\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right) = 1 - \\alpha  \n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided upper confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}, \\infty \\right)\n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided lower confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nLet us again consider the previous problem but let us now suppose that when the value \\(\\mu\\) is transmitted at location A then the value received at location B is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) but with \\(\\sigma^2\\) being unknown. If 9 successive values are, 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, and 10.5, compute a 95 percent confidence interval for \\(\\mu\\).\n\n\n\n\n2.2.3 Confidence Intervals for the Variance of a Normal Distribution\n\nIf we are sampling from a normal distribution with unknown mean and unknown variance then;\n\n\\[\n(n-1) \\frac{S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nfollows a chi-squared distribution.\n\nWe have\n\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}  \\right) = 1 - \\alpha\n\\]\n\nHence, 100(1-\\(\\alpha\\)) percent two-sided confidence interval for the variance in this case;\n\n\\[\n\\sigma^2 \\in \\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\n\\]\n\nThe 100(1-\\(\\alpha\\)) percent one-sided upper and lower confidence intervals in this case will be respectively;\n\n\\[\n\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha, n-1}}, \\infty \\right)\n\\]\nand\n\\[\n\\left( 0, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha, n-1}} \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nA standardized procedure is expected to produce washers with very small deviation in their thicknesses. Suppose that 10 such washers were chosen and measured. If the thicknesses of these washers were, in inches; .123, .133, .124, .125, .126, .128, .120, .124, .130, and .126. What is a 90 percent confidence interval for the standard deviation of the thickness of a washer produced by this procedure?\n\n\nAll problems and most part of text are taken from Ross (2009) .\n\n\n\n\nRoss, Sheldon. 2009. “Probability and Statistics for Engineers and Scientists.” Elsevier, New Delhi 16: 32–33.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html",
    "href": "HypothesisTesting.html",
    "title": "3  Hypothesis Testing",
    "section": "",
    "text": "Introduction\n\\[\nH_{0}: \\theta = 1\n\\]\n\\[\nH_{0}: \\theta &gt; 1\n\\]\n\\[\nH_{0}: \\theta \\leq 1\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html#with-known-variance",
    "href": "HypothesisTesting.html#with-known-variance",
    "title": "3  Hypothesis Testing",
    "section": "4.1 With known Variance",
    "text": "4.1 With known Variance\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nWe are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nSince \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) is a natural point estimator of \\(\\mu\\), it is reasonable to accept \\(H_0\\) if \\(\\bar{X}\\) is not too far from \\(\\mu_0\\).\nThus, the critical region of the test would be of the form:\n\n\\[\nC = \\{X_1, \\ldots, X_n : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nfor some suitably chosen value \\(c\\).\n\nTo ensure that the test has a significance level \\(\\alpha\\), we must determine the critical value \\(c\\) in the above equation such that the type I error is equal to \\(\\alpha\\). This means \\(c\\) must satisfy:\n\n\\[\nP_{\\mu_0} \\{|\\bar{X} - \\mu_0| &gt; c\\} = \\alpha\n\\]\nwhere \\(P_{\\mu_0}\\) denotes that the probability is computed under the assumption that population mean, \\(\\mu = \\mu_0\\).\n\nWhen \\(\\mu = \\mu_0\\), \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\frac{\\sigma^2}{n}\\). Therefore, the standardized variable \\(Z\\) defined by:\n\n\\[\nZ = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n\\]\nwill have a standard normal distribution.\n\nThe probability of a type I error is given by:\n\n\\[\nP \\left( |\\bar{X} - \\mu_0| &gt; c \\right) = \\alpha\n\\]\n\nEquivalently, this can be written as:\n\n\\[\n2P \\left( Z &gt; \\frac{c \\sqrt{n}}{\\sigma} \\right) = \\alpha\n\\]\n\nWhere \\(Z\\) is a standard normal random variable. We know that:\n\n\\[\nP \\left( Z &gt; z_{\\alpha/2} \\right) = \\frac{\\alpha}{2}\n\\]\n\nTherefore, we have:\n\n\\[\n\\frac{c \\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\n\nSolving for \\(c\\), we get:\n\n\\[\nc = \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nThus, the test at significance level \\(\\alpha\\) is to reject \\(H_0\\) if:\n\n\\[\n|\\bar{X} - \\mu_0| &gt; \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nAnd accept \\(H_0\\) otherwise. Equivalently, we can reject \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} &gt; z_{\\alpha/2}\n\\]\n\nAnd accept \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} \\leq z_{\\alpha/2}\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nIf a signal of value \\(\\mu\\) is sent from location A, then the value received at location B is normally distributed with mean \\(\\mu\\) and standard deviation 2. That is, the random noise added to the signal is an N(0, 4) random variable. There is reason for the people at location B to suspect that the signal value \\(\\mu\\) = 8 will be sent today. Test this hypothesis if the same signal value is independently sent five times and the average value received at location B is X = 9. 5.\n\n\n\n4.1.1 Choosing the Significance Level\n\nThe appropriate significance level \\(\\alpha\\) depends on the specific context and consequences of the hypothesis test.\nIf rejecting the null hypothesis \\(H_0\\) would lead to significant costs or consequences, a more conservative significance level (e.g., 0.05 or 0.01) should be chosen.\nIf there is a strong initial belief that \\(H_0\\) is true, strict evidence is required to reject \\(H_0\\), implying a lower significance level.\nThe test can be described as follows: For an observed value of the test statistic \\(\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma}\\), denoted as \\(v\\), reject \\(H_0\\) if the probability of the test statistic being as large as \\(v\\) under \\(H_0\\) is less than or equal to \\(\\alpha\\).\nThis probability is known as the p-value of the test. \\(H_0\\) is accepted if \\(\\alpha\\) is less than the p-value and rejected if \\(\\alpha\\) is greater than or equal to the p-value.\nIn practice, the significance level is sometimes not set in advance. Instead, the p-value is calculated from the data, and decisions are made based on the p-value.\nIf the p-value is much larger than any reasonable significance level, \\(H_0\\) is accepted. Conversely, if the p-value is very small, \\(H_0\\) is rejected.\n\n\n\n4.1.2 Hypothesis Testing Summary for \\(N(\\mu, \\sigma^2)\\) Population\n\nCaption: Summary of hypothesis testing for a sample from a \\(N(\\mu, \\sigma^2)\\) population with known \\(\\sigma^2\\).\n\n\n\n\n\n\nSample and Population\nDetails\n\n\n\n\nSample\n\\(\\{X_1, X_2, . . . , X_n\\}\\)\n\n\nPopulation\n\\(N(\\mu, \\sigma^2)\\)\n\n\nKnown Parameter\n\\(\\sigma^2\\)\n\n\nSample Mean\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\n\n\nSignificance Level\n\\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nTest Statistic (TS)\nReject if\np-Value if TS = t\n\n\n\n\n\\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(|TS| &gt; z_{\\alpha/2}\\)\n\\(2P\\{Z \\geq |t|\\}\\)\n\n\n\\(H_0: \\mu \\leq \\mu_0\\) vs \\(H_1: \\mu &gt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &gt; z_{\\alpha}\\)\n\\(P\\{Z \\geq t\\}\\)\n\n\n\\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu &lt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &lt; -z_{\\alpha}\\)\n\\(P\\{Z \\leq t\\}\\)\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nImagine you’re the quality control manager at a company that prides itself on the precision of its product weights. The company claims that the average weight of their product is exactly 100 grams. But, as a diligent manager, you decide to put this claim to the test. You randomly select a sample of 30 products and measure their weights. To your surprise, the average weight of your sample is 110 grams! Now, you need to determine if this difference is statistically significant or just a fluke. Assume the population standard deviation as 15 grams.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  }
]