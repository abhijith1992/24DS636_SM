[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling - 24DS636 (2024-25)",
    "section": "",
    "text": "Course Introduction\nThe website contains course contents of “Statistical Modeling” offered by Abhijith M S, PhD to Masters students pursuing M.Tech in Data Science, during the even semester of the academic year 2024-25.\n\n\nSyllabus\n(As given in the curriculum)\n\nProbability, Random Variables & Probability Distributions.\nSampling, analysis of sample data-Empirical Distributions, Sampling from a Population Estimation, confidence intervals, point estimation–Maximum Likelihood, Probability mass functions, Modeling distributions, Hypothesis testing- Z, t, Chi-Square.\nANOVA & Designs of Experiments - Single, Two factor ANOVA, Factorials ANOVA models.\nLinear least squares, Correlation & Regression Models-linear regression methods, Ridge regression, LASSO, univariate and Multivariate Linear Regression, probabilistic interpretation, Regularization, Logistic regression, locally weighted regression.\nExploratory data analysis, Time series analysis, Analytical methods – ARIMA and SARIMA.\n\n\n\nEvaluations: A Tentative Timeline\n\nBest two marks out of three quizzes (Total = 20 marks)\nQuiz-1 (10 marks): (January First week)\nQuiz-2 (10 marks):(March First week)\nQuiz-3 (10 marks):(April First week)\nAssignments (Total = 30 marks)\nAssignment-1 (10 marks):(Submission: End of January)\nAssignment-2 (10 marks):(Submission: End of March)\nProject Review - 1 (10 marks):(February second week)\nMid Sem (Total = 20 marks)\nMid-Semester Exam (20 marks):(Feb first week, as per Academic calender)\nEnd Sem (Total = 30 marks)\nEnd-Semester Project Presentation (20 marks):(April second week, as per Academic calender) \\end{itemize}$\n\nContact: ms_abhijith@cb.amrita.edu",
    "crumbs": [
      "Course Introduction"
    ]
  },
  {
    "objectID": "ParameterEstimation.html",
    "href": "ParameterEstimation.html",
    "title": "1  Parameter Estimation",
    "section": "",
    "text": "1.1 Point Estimation: Maximum Likelihood Estimators",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html#interval-estimates",
    "href": "ParameterEstimation.html#interval-estimates",
    "title": "1  Parameter Estimation",
    "section": "1.2 Interval Estimates",
    "text": "1.2 Interval Estimates\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a known distribution with an unknown mean \\(\\mu\\).\nIt is established that the sample mean \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) serves as the maximum likelihood estimator for \\(\\mu\\).\nHowever, the sample mean \\(\\bar{X}\\) is not expected to be exactly equal to \\(\\mu\\), but rather close to it.\nTherefore, instead of providing a single point estimate, it is often more useful to specify an interval within which we are confident that \\(\\mu\\) lies.\nTo determine such an interval estimator, we utilize the probability distribution of the point estimator.\n\n\n1.2.1 Confidence Intervals for the Mean of a normal population with known Variance\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nThe point estimator \\(\\bar{X}\\) is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\)/n.\nTherefore, \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\) follows a standard normal distribution.\n\n\n\n\n\n\n\nWhat to do\n\n\n\nConsider that I want to find an interval around \\(\\bar{X}\\) such that the actual population mean \\(\\mu\\) falls within the interval, say 95 % of the times.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor finding such an interval, I can use the Z-table. From the Z-table I can find:\n\n\\[\nP\\left( -1.96 &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96 \\right) = 0.9750 - 0.0250 = 0.95\n\\]\n\nRewriting the above equation:\n\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} -\\mu &lt; 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( 1.96\\frac{\\sigma}{\\sqrt{n}} &gt; \\mu - \\bar{X} &gt; -1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu - \\bar{X} &lt; 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\nWe have P(Z &lt; -1.96) = 0.025, similarly P(Z &gt; 1.96) = 0.025. Usually 1.96 is represented generally as \\(z_{0.025}\\). Thus, P(Z &lt; -z\\(_{0.025}\\)) = 0.025 and P(Z &gt; z\\(_{0.025}\\)) = 0.025.\nHence, 100(1-0.05) percent confidence interval for the mean of a normal population with known variance is:\n\n\\[\nP\\left( \\bar{X} - z_{0.025}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}}\\right) = (1 - 0.05)\n\\]\n\n\n\nFor a confidence level of \\(100(1-\\alpha)\\) percent, the corresponding critical value from the standard normal distribution is \\(z_{\\alpha/2}\\).\nThe \\(100(1-\\alpha)\\) percent confidence interval for \\(\\mu\\) is given by:\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{1.1}\\]\n\nThe interval as given in Equation 1.1 is called a two-sided confidence interval.\nAlso the term \\(z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\) is called the margin of error.\n\n\n\n\n\n\n\nDerivation of two-sided confidence interval\n\n\n\n\nTo find 100(1-\\(\\alpha\\)) percent confidence interval of mean (\\(\\mu\\)), we have;\n\n\\[\nP\\left( -z_{\\alpha/2} &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; z{_\\alpha/2} \\right) = 1 - \\alpha\n\\]\n\nDoing the same manipulations we did earlier for obtaining the 95percent confidence interval we can obtain:\n\n\\[\nP\\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nThe above equation give us the required confidence interval, as given in Equation 1.1.\n\n\n\n\n\n\n\n\n\nWhat if !?\n\n\n\nWhat if we are interested in one sided confidence intervals !!?\n\n\n\n\n\n\n\n\nOne-sided Upper Confidence Iterval\n\n\n\n\nTo determine such an interval, for a standard normal random variable Z, we have;\n\n\\[\nP\\left( Z &lt; 1.645 \\right) = 0.95\n\\]\n\nThus, \\[\nP\\left( \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} &lt; 1.645 \\right) = 0.95\n\\]\n\n\\[\nP\\left( \\mu -\\bar{X} &gt; - 1.645\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}  \\right) = 0.95\n\\]\n\nThus a 95 percent one-sided upper confidence interval for \\(\\mu\\) is\n\n\\[\n\\mu \\in \\left( \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\nor in other words; 100(1-0.05) percent one-sided upper confidence interval for \\(\\mu\\) is\n\\[\n\\mu \\in \\left( \\bar{X} - z_{0.05}\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\n\n\n\n\n\n\n\n\nOneside interval!\n\n\n\nCan you think of another one sided confidence interval?\n\n\n\n\n\n\n\n\nOne-sided lower confidence interval\n\n\n\n\nWe have \\[\nP\\left( Z &gt; - 1.645 \\right) = 0.95\n\\]\nProceed just like in the previous case and you will find a 100(1-0.05) percent one-sided lower confidence interval for \\(\\mu\\) as;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{0.05}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\]\n\n\n\nIn general, 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval for \\(\\mu\\) is given in Equation 1.2.\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}, \\infty \\right)\n\\tag{1.2}\\]\n\nAlso, 100(1-\\(\\alpha\\))percent one-sided lower confidence interval for \\(\\mu\\) is given in Equation 1.3. \\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{1.3}\\]\nThe python code below creates a sample and find 95% confidence interval for the mean if the population standard deviation is assumed to be 10. Other values are specified in the code.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 50  # true mean\nsigma = 10  # known standard deviation\nn = 30  # sample size\nalpha = 0.05  # significance level\n\n# Generate a sample\nnp.random.seed(0)\nsample = np.random.normal(mu, sigma, n)\nsample_mean = np.mean(sample)\n\n# Calculate the confidence interval\nz = 1.96  # z-value for 95% confidence\nmargin_of_error = z * (sigma / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\n# Plot the sample and confidence interval\nplt.figure(figsize=(8, 4))\nplt.hist(sample, bins=10, alpha=0.7, color='blue', edgecolor='black')\nplt.axvline(sample_mean, color='red', linestyle='dashed', linewidth=2, label='Sample Mean')\nplt.axvline(confidence_interval[0], color='green', linestyle='dashed', linewidth=2, label='95% CI Lower Bound')\nplt.axvline(confidence_interval[1], color='green', linestyle='dashed', linewidth=2, label='95% CI Upper Bound')\nplt.title('Sample Distribution with 95% Confidence Interval')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"95% Confidence Interval: {confidence_interval}\")\n\n\n\n\n\n\n\n\nSample Mean: 54.42856447263174\n95% Confidence Interval: (50.85011043026466, 58.007018514998826)\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose that when a signal having value \\(\\mu\\) is transmitted from location A the value received at location B is normally distributed with mean \\(\\mu\\) and variance 4. That is, if \\(\\mu\\) is sent, then the value received is \\(\\mu\\) + N where N, representing noise, is normal with mean 0 and variance 4. To reduce error, suppose the same value is sent 9 times. If the successive values received are 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5;\n(a). construct a 95 percent two-sided confidence interval for \\(\\mu\\).\n(b). construct 95 percent one-sided upper and lower confidence intervals for \\(\\mu\\).\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose a quality control manager at a factory wants to ensure that the average weight of a product is at least 500 grams. They take a random sample of 30 products and find the sample mean weight to be 495 grams with a standard deviation of 10 grams. Help the manager to estimate the minimum average weight of the products with 95% confidence.\n\n\n\n\n1.2.2 Confidence Intervals for the Mean of a normal population with unknown Variance\n\nIf you recollect the discussion we had about the sample mean from a normal population with unknown variance we saw that variable t\\(_{n-1}\\) given by:\n\n\\[\nt_{n-1} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{S}\n\\]\nhas a t-distribution with n-1 degrees of freedom.\n\nBecause of the symmetry of the t-distribution we can write for any \\(\\alpha\\) \\(\\in\\) (0, 1/2);\n\n\\[\nP\\left( -t_{\\alpha/2, n-1} &lt; \\sqrt{n}\\frac{\\bar{X} - \\mu}{S} &lt;  t_{\\alpha/2, n-1} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( -\\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &lt;  - \\mu &lt; -\\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &gt;   \\mu &gt; \\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nIf the sample mean is \\(\\bar{X}\\) and sample standard deviation S, then we can say that with 100(1-\\(\\alpha\\)) percent confidence that\n\n\\[\n\\mu \\in \\left(\\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}},  \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} \\right)\n\\]\n\nIn this case 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval can be obtained from the fact that:\n\n\\[\nP\\left( \\sqrt{n}\\frac{(\\bar{X} - \\mu)}{S} &lt; t_{\\alpha, n-1}\\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right) = 1 - \\alpha  \n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided upper confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}, \\infty \\right)\n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided lower confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nLet us again consider the previous problem but let us now suppose that when the value \\(\\mu\\) is transmitted at location A then the value received at location B is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) but with \\(\\sigma^2\\) being unknown. If 9 successive values are, 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, and 10.5, compute a 95 percent confidence interval for \\(\\mu\\).\n\n\n\n\n1.2.3 Confidence Intervals for the Variance of a Normal Distribution\n\nIf we are sampling from a normal distribution with unknown mean and unknown variance then;\n\n\\[\n(n-1) \\frac{S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nfollows a chi-squared distribution.\n\nWe have\n\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}  \\right) = 1 - \\alpha\n\\]\n\nHence, 100(1-\\(\\alpha\\)) percent two-sided confidence interval for the variance in this case;\n\n\\[\n\\sigma^2 \\in \\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\n\\]\n\nThe 100(1-\\(\\alpha\\)) percent one-sided upper and lower confidence intervals in this case will be respectively;\n\n\\[\n\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha, n-1}}, \\infty \\right)\n\\]\nand\n\\[\n\\left( 0, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha, n-1}} \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nA standardized procedure is expected to produce washers with very small deviation in their thicknesses. Suppose that 10 such washers were chosen and measured. If the thicknesses of these washers were, in inches; .123, .133, .124, .125, .126, .128, .120, .124, .130, and .126. What is a 90 percent confidence interval for the standard deviation of the thickness of a washer produced by this procedure?\n\n\nAll problems and most part of text are taken from Ross (2009) .\n\n\n\n\nRoss, Sheldon. 2009. “Probability and Statistics for Engineers and Scientists.” Elsevier, New Delhi 16: 32–33.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html",
    "href": "HypothesisTesting.html",
    "title": "2  Hypothesis Testing",
    "section": "",
    "text": "Problem\n\n\n\nA quality control manager at a factory wants to ensure that the average weight of a product is at least 500 grams. They take a random sample of 30 products and find the sample mean weight to be 495 grams with a standard deviation of 10 grams. The manager wants to test if the average weight of the products is significantly less than 500 grams at a 5% significance level.\n\nNull Hypothesis (H₀): The average weight of the products is at least 500 grams (\\(\\mu\\) \\(\\geq\\) 500).\nAlternative Hypothesis (H₁): The average weight of the products is less than 500 grams (\\(\\mu\\) &lt; 500).\n\n\n\n\n\n\n\n\n\nExamples Highlighting the Need for Hypothesis Testing\n\n\n\n\nMedical Research:\n\nScenario: A pharmaceutical company develops a new drug intended to lower blood pressure.\nHypothesis Testing: The null hypothesis (\\(H_0\\)) might state that the new drug has no effect on blood pressure, while the alternative hypothesis (\\(H_1\\)) states that the drug does lower blood pressure. Hypothesis testing helps determine if the observed effects in clinical trials are statistically significant or if they could have occurred by random chance.\n\nQuality Control:\n\nScenario: A factory produces light bulbs, and the quality control team wants to ensure that the average lifespan of the bulbs is 1000 hours.\nHypothesis Testing: The null hypothesis (\\(H_0\\)) could be that the mean lifespan of the bulbs is 1000 hours. The alternative hypothesis (\\(H_1\\)) might be that the mean lifespan is not 1000 hours. Hypothesis testing helps the team decide whether to accept the production process or take corrective actions.\n\nMarketing:\n\nScenario: A company launches a new advertising campaign and wants to know if it has increased sales.\nHypothesis Testing: The null hypothesis (\\(H_0\\)) might state that the advertising campaign has no effect on sales, while the alternative hypothesis (\\(H_1\\)) states that the campaign has increased sales. Hypothesis testing helps the company determine if the increase in sales is statistically significant.\n\nEducation:\n\nScenario: An educator wants to test if a new teaching method is more effective than the traditional method.\nHypothesis Testing: The null hypothesis (\\(H_0\\)) could be that there is no difference in effectiveness between the new and traditional methods. The alternative hypothesis (\\(H_1\\)) might be that the new method is more effective. Hypothesis testing helps in making data-driven decisions about adopting new teaching strategies.\n\nEnvironmental Science:\n\nScenario: Researchers want to determine if a new policy has reduced pollution levels in a city.\nHypothesis Testing: The null hypothesis (\\(H_0\\)) might state that the policy has no effect on pollution levels, while the alternative hypothesis (\\(H_1\\)) states that the policy has reduced pollution levels. Hypothesis testing helps in evaluating the effectiveness of environmental policies.\n\n\nThese examples illustrate how hypothesis testing is a crucial tool in various fields for making informed decisions based on data.\n\n\n\n\n\n\n\n\nImportant Terminology\n\n\n\n\nNull Hypothesis (\\(H_0\\)): The hypothesis that there is no effect or no difference. It is the default assumption that any observed effect is due to random chance. It is the hypothesis that researchers aim to test against.\nAlternative Hypothesis (\\(H_1\\) or \\(H_a\\)): The hypothesis that there is an effect or a difference. It is what researchers want to prove.\nTest Statistic: A standardized value that is calculated from sample data during a hypothesis test. It is used to decide whether to reject the null hypothesis.\nP-value: The probability of obtaining test results at least as extreme as the observed results, assuming that the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis.\nSignificance Level (\\(\\alpha\\)): A threshold set by the researcher which the p-value must be below in order to reject the null hypothesis. Common significance levels are 0.05, 0.01, and 0.10.\nCritical Value: The value that the test statistic must exceed in order to reject the null hypothesis. It is determined based on the significance level and the distribution of the test statistic.\nPower of a Test: The probability that the test correctly rejects a false null hypothesis (i.e., it does not make a type II error). Higher power indicates a greater ability to detect an effect when there is one.\nType I Error: The error made when the null hypothesis is true, but is incorrectly rejected. The probability of making a type I error is denoted by \\(\\alpha\\).\nType II Error: The error made when the null hypothesis is false, but is incorrectly accepted. The probability of making a type II error is denoted by \\(\\beta\\).\nConfidence Interval: A range of values derived from the sample data that is likely to contain the population parameter. It provides an estimate of the parameter with a certain level of confidence (e.g., 95%).\nOne-tailed Test: A hypothesis test in which the region of rejection is on only one side of the sampling distribution. It tests for the possibility of the relationship in one direction.\nTwo-tailed Test: A hypothesis test in which the region of rejection is on both sides of the sampling distribution. It tests for the possibility of the relationship in both directions.\n\n\n\n\n2.0.1 Introduction and Important Terminology\n\nA statistical hypothesis is typically a statement regarding a set of parameters of a population distribution.\nIt is termed a hypothesis because its true value is unknown.\nThe main challenge is to devise a method to determine whether the values of a random sample from this population align with the hypothesis.\nConsider a population with distribution \\(F_\\theta\\), where \\(\\theta\\) is unknown.\nWe aim to test a specific hypothesis about \\(\\theta\\).\nThis hypothesis is denoted by \\(H_0\\) and is referred to as the null hypothesis.\nFor instance, if \\(F_\\theta\\) is a normal distribution function with mean \\(\\theta\\) and variance equal to 1, two possible null hypotheses about \\(\\theta\\) are:\n\n\\[\nH_{0}: \\theta = 1\n\\]\n\\[\nH_{0}: \\theta &gt; 1\n\\]\n\\[\nH_{0}: \\theta \\leq 1\n\\]\n\nIt is important to note that the null hypothesis in the first case fully specifies the population distribution.\nWhereas the null hypothesis in the second and third cases do not.\n\n\n\n\n\n\n\nSimple and Composite Hypotheses\n\n\n\n\nA hypothesis that fully specifies the population distribution when true is known as a simple hypothesis. (Eg; \\(H_{0}: \\theta = 1\\))\nA hypothesis that does not fully specifies the population distribution is referred to as a composite hypothesis. (Eg; \\(H_{0}: \\theta &gt; 1\\), H_{0}: )\n\n\n\n\n\n2.0.2 Testing a Null Hypothesis\n\nTo test a specific null hypothesis \\(H_0\\), we take a sample of size \\(n\\) from the population, say \\(X_1, X_2, \\ldots, X_n\\).\nBased on these \\(n\\) values, we decide whether to accept or reject \\(H_0\\).\nWe define a region \\(C\\) in the \\(n\\)-dimensional space. This region is called the critical region.\nIf the sample \\(X_1, X_2, \\ldots, X_n\\) falls within the critical region \\(C\\), we reject \\(H_0\\). Otherwise, we accept \\(H_0\\).\nIn simple terms, the critical region \\(C\\) helps us determine the outcome of the statistical test.\n\n\\[\naccepts \\,\\,\\, H_0 \\,\\,\\, if \\,\\,\\, \\left( X_1, X_2, . . . , X_n \\right) \\,\\,\\, \\notin \\,\\,\\, C\n\\]\nand\n\\[\nrejects \\,\\,\\, H_0 \\,\\,\\, if \\,\\,\\, \\left( X_1, X_2, . . . , X_n \\right) \\,\\,\\, \\in \\,\\,\\, C\n\\]\n\n\n\n\n\n\nTypes of Errors in Hypothesis Testing\n\n\n\n\nWhen developing a procedure for testing a given null hypothesis \\(H_0\\), it is crucial to recognize that two different types of errors can occur.\nA type I error occurs if the test incorrectly rejects \\(H_0\\) when it is actually true.\nA type II error occurs if the test incorrectly accepts \\(H_0\\) when it is actually false.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe goal of a statistical test for \\(H_0\\) is not to definitively determine its truth but to assess if the data is consistent with \\(H_0\\).\n\n\n\n\n2.0.3 Significance Level and Classical Approach\n\n\\(H_0\\) should be rejected only if the observed data is highly unlikely under \\(H_0\\).\nThe classical method involves specifying a value \\(\\alpha\\), known as the level of significance.\nThe test is designed so that the probability of rejecting \\(H_0\\) when it is true does not exceed \\(\\alpha\\).\nCommon choices for \\(\\alpha\\) are 0.1, 0.05, and 0.005.\nThis approach ensures that the probability of a type I error (incorrectly rejecting \\(H_0\\)) is controlled and does not exceed the chosen \\(\\alpha\\).\n\n\n\n2.0.4 Testing Hypotheses with Point Estimators\n\nSuppose we are interested in testing a hypothesis concerning \\(\\theta\\), an unknown parameter of the population.\nSpecifically, for a given set of parameter values \\(w\\), we aim to test:\n\n\\[\nH_0 : \\theta \\in w\n\\]\n\nA common approach to developing a test of \\(H_0\\) at a significance level \\(\\alpha\\) is to start by determining a point estimator of \\(\\theta\\), say \\(d(X)\\).\nThe hypothesis \\(H_0\\) is then rejected if \\(d(X)\\) is “far away” from the region \\(w\\).\nTo determine how “far away” it needs to be to justify rejection of \\(H_0\\), we need to determine the probability distribution of \\(d(X)\\) when \\(H_0\\) is true.\nThis distribution helps us determine the appropriate critical region to ensure the test has the required significance level \\(\\alpha\\).\n\n\n\n\n\n\n\nExample\n\n\n\n\nFor instance, consider testing the hypothesis that the mean of a normal distribution with parameters \\((\\theta, 1)\\) is equal to 1.\nThe test rejects the null hypothesis if the point estimate of \\(\\theta\\) (i.e., the sample mean) deviates more than \\(\\frac{1.96}{\\sqrt{n}}\\) from 1.\nAs we will discuss in the next section, the value \\(\\frac{1.96}{\\sqrt{n}}\\) is selected to achieve a significance level of \\(\\alpha = 0.05\\).\n\n\n\n\n\n2.0.5 Hypothesis Tests Concerning the mean of a normal population\n\n2.0.5.1 With known Variance\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nWe are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nSince \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) is a natural point estimator of \\(\\mu\\), it is reasonable to accept \\(H_0\\) if \\(\\bar{X}\\) is not too far from \\(\\mu_0\\).\nThus, the critical region of the test would be of the form:\n\n\\[\nC = \\{X_1, \\ldots, X_n : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nfor some suitably chosen value \\(c\\).\n\nTo ensure that the test has a significance level \\(\\alpha\\), we must determine the critical value \\(c\\) in the above equation such that the type I error is equal to \\(\\alpha\\). This means \\(c\\) must satisfy:\n\n\\[\nP_{\\mu_0} \\{|\\bar{X} - \\mu_0| &gt; c\\} = \\alpha\n\\]\nwhere \\(P_{\\mu_0}\\) denotes that the probability is computed under the assumption that population mean, \\(\\mu = \\mu_0\\).\n\nWhen \\(\\mu = \\mu_0\\), \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\frac{\\sigma^2}{n}\\). Therefore, the standardized variable \\(Z\\) defined by:\n\n\\[\nZ = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n\\]\nwill have a standard normal distribution.\n\nThe probability of a type I error is given by:\n\n\\[\nP \\left( |\\bar{X} - \\mu_0| &gt; c \\right) = \\alpha\n\\]\n\nEquivalently, this can be written as:\n\n\\[\n2P \\left( Z &gt; \\frac{c \\sqrt{n}}{\\sigma} \\right) = \\alpha\n\\]\n\nWhere \\(Z\\) is a standard normal random variable. We know that:\n\n\\[\nP \\left( Z &gt; z_{\\alpha/2} \\right) = \\frac{\\alpha}{2}\n\\]\n\nTherefore, we have:\n\n\\[\n\\frac{c \\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\n\nSolving for \\(c\\), we get:\n\n\\[\nc = \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nThus, the test at significance level \\(\\alpha\\) is to reject \\(H_0\\) if:\n\n\\[\n|\\bar{X} - \\mu_0| &gt; \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nAnd accept \\(H_0\\) otherwise. Equivalently, we can reject \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} &gt; z_{\\alpha/2}\n\\]\n\nAnd accept \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} \\leq z_{\\alpha/2}\n\\]\n\n\n\n2.0.6 Choosing the Significance Level\n\nThe appropriate significance level \\(\\alpha\\) depends on the specific context and consequences of the hypothesis test.\nIf rejecting the null hypothesis \\(H_0\\) would lead to significant costs or consequences, a more conservative significance level (e.g., 0.05 or 0.01) should be chosen.\nIf there is a strong initial belief that \\(H_0\\) is true, strict evidence is required to reject \\(H_0\\), implying a lower significance level.\nThe test can be described as follows: For an observed value of the test statistic \\(\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma}\\), denoted as \\(v\\), reject \\(H_0\\) if the probability of the test statistic being as large as \\(v\\) under \\(H_0\\) is less than or equal to \\(\\alpha\\).\nThis probability is known as the p-value of the test. \\(H_0\\) is accepted if \\(\\alpha\\) is less than the p-value and rejected if \\(\\alpha\\) is greater than or equal to the p-value.\nIn practice, the significance level is sometimes not set in advance. Instead, the p-value is calculated from the data, and decisions are made based on the p-value.\nIf the p-value is much larger than any reasonable significance level, \\(H_0\\) is accepted. Conversely, if the p-value is very small, \\(H_0\\) is rejected.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  }
]