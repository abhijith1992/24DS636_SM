[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling - 24DS636 (2024-25)",
    "section": "",
    "text": "Course Introduction\nThe website contains course contents of “Statistical Modeling” offered by Abhijith M S, PhD to Masters students pursuing M.Tech in Data Science, during the even semester of the academic year 2024-25.\n\n\nSyllabus\n(As given in the curriculum)\n\nProbability, Random Variables & Probability Distributions.\nSampling, analysis of sample data-Empirical Distributions, Sampling from a Population Estimation, confidence intervals, point estimation–Maximum Likelihood, Probability mass functions, Modeling distributions, Hypothesis testing- Z, t, Chi-Square.\nANOVA & Designs of Experiments - Single, Two factor ANOVA, Factorials ANOVA models.\nLinear least squares, Correlation & Regression Models-linear regression methods, Ridge regression, LASSO, univariate and Multivariate Linear Regression, probabilistic interpretation, Regularization, Logistic regression, locally weighted regression.\nExploratory data analysis, Time series analysis, Analytical methods – ARIMA and SARIMA.\n\n\n\nEvaluations: A Tentative Timeline\n\nBest two marks out of three quizzes (Total = 20 marks)\nQuiz-1 (10 marks): (January First week)\nQuiz-2 (10 marks):(March First week)\nQuiz-3 (10 marks):(April First week)\nAssignments (Total = 30 marks)\nAssignment-1 (10 marks):(Submission: End of January)\nAssignment-2 (10 marks):(Submission: End of March)\nProject Review - 1 (10 marks):(February second week)\nMid Sem (Total = 20 marks)\nMid-Semester Exam (20 marks):(Feb first week, as per Academic calender)\nEnd Sem (Total = 30 marks)\nEnd-Semester Project Presentation (20 marks):(April second week, as per Academic calender) \\end{itemize}$\n\nContact: ms_abhijith@cb.amrita.edu",
    "crumbs": [
      "Course Introduction"
    ]
  },
  {
    "objectID": "DescriptiveStatistics.html",
    "href": "DescriptiveStatistics.html",
    "title": "1  Descriptive statistics",
    "section": "",
    "text": "Descriptive statistics deals with methods to describe and summarize data.\nDescribing of data is effectively done through tables or graphs. Those often reveal important features such as the range, the degree of concentration, and the symmetry of the data.\nThe summary of data is expressed through numerical quantities (summary statistics) whose values are determined by the data.\n\n\n1.0.1 Describing Data sets\n\n\n1.0.2 Frequency Tables and Graphs\n\nA data set having a relatively small number of distinct values can be conveniently presented in a frequency table.\nData from a frequency table can be graphically represented by:\n\nLine Graph\nBar Graph\nFrequency Polygon\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\n# Line Graph\nplt.plot(values, frequencies, marker='o')\nplt.title('Frequency Polygon')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.show()\n\n\nplt.bar(values, frequencies, width=0.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0.3 Relative Frequency Tables and Graphs\n\nConsider a data set consisting of n values. If f is the frequency of a particular value, then the ratio f /n is called its relative frequency.\nThat is, the relative frequency of a data value is the proportion of the data that have that value.\nThe relative frequencies can be represented graphically by:\n\nrelative frequency line\nrelative frequency bar graph\nrelative frequency polygon\npie chart: A pie chart is often used to indicate relative frequencies when the data are not numerical in nature. A circle is constructed and then sliced into different sectors with areas proportional to the respective relative frequencies.\n\n\n\nimport numpy as np \nimport matplotlib.pyplot as plt \n\n# Sample data\ndata = np.array(['A', 'B', 'C', 'A', 'A', 'B','B','B','B','B','B','C','C','C','C'])\n\n# Calculate frequencies\nvalues, frequencies = np.unique(data, return_counts=True)\n\nrelative_frequencies = frequencies/len(data)\nprint(relative_frequencies)\n\n\nplt.pie(relative_frequencies, labels = values, autopct='%1.1f%%')\nplt.show()\n\n[0.2        0.46666667 0.33333333]\n\n\n\n\n\n\n\n\n\n\n\n1.0.4 Grouped Data, Histograms, Ogives, and Stem and Leaf Plots\n\nFor some data sets the number of distinct values is too large to utilize frequency tables.\nInstead, in such cases, it is useful to divide the values into groupings, or class intervals, and then plot the number of data values falling in each class interval.\nThe number of class intervals chosen should be a trade-off between:\n\nchoosing too few classes at a cost of losing too much information about the actual data values in a class.\nchoosing too many classes, which will result in the frequencies of each class being too small.\n\nIt is common, although not essential, to choose class intervals of equal length.\nThe endpoints of a class interval are called the class boundaries.\nWe will adopt the left-end inclusion convention, which stipulates that a class interval contains its left-end but not its right-end boundary point.\nThus, for instance, the class interval 20-30 contains all values that are both greater than or equal to 20 and less than 30.\nA bar graph plot of class data, with the bars placed adjacent to each other, is called a histogram.\nThe vertical axis of a histogram can represent either the class frequency or the relative class frequency.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\n# Plot histogram\nplt.hist(random_integers, bins=bins, edgecolor='black')\nplt.title('Histogram of Random Integers')\nplt.xlabel('Class Interval')\nplt.ylabel('Frequency')\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n\n\n\n\n\n\n\n\n\n\nWe are sometimes interested in plotting a cumulative frequency (or cumulative relative frequency) graph.\nA point on the horizontal axis of such a graph represents a possible data value; its corresponding vertical plot gives the number (or proportion) of the data whose values are less than or equal to it.\nA cumulative frequency plot is called an ogive.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(50)\n\n# Generate 300 random integers between 100 and 1500\nrandom_integers = np.random.randint(100, 1501, size=300)\n\n# Create bins for the class intervals\nbins = np.arange(100, 1600, 100)\nprint(bins)\n\nhistograms = np.histogram(random_integers, bins=bins)[0]\nprint(histograms)\ncumulativeSum = np.cumsum(histograms) \nprint(cumulativeSum)\n\n\nplt.plot(bins[:-1], cumulativeSum, marker='o', linestyle='-')\nplt.title('Ogive (Cumulative Frequency Graph)')\nplt.xlabel('Class Interval')\nplt.ylabel('Cumulative Frequency')\n#plt.grid(True)\nplt.show()\n\n[ 100  200  300  400  500  600  700  800  900 1000 1100 1200 1300 1400\n 1500]\n[22 25 15 24 14 21 20 24 19 21 23 21 28 23]\n[ 22  47  62  86 100 121 141 165 184 205 228 249 277 300]\n\n\n\n\n\n\n\n\n\n\nAn efficient way of organizing a small-to moderate-sized data set is to utilize a stem and leaf plot.\nSuch a plot is obtained by first dividing each data value into two parts - its stem and its leaf.\nFor instance, if the data are all two-digit numbers, then we could let the stem part of a data value be its tens digit and let the leaf be its ones digit.\nThus, for instance, the value 62 is expressed as\n\n\nimport numpy as np\n\n# Sample data\ndata = np.array([62, 67, 63, 68, 69, 61, 64, 65, 66, 60, 75, 74, 76, 78, 90, 92, 34, 36, 56, 57, 45, 53, 52, 59, 73, 74, 79, 80, 81,20, 34, 36, 85])\n\n# Create stem and leaf plot\nstem_leaf = {}\n\nfor number in data:\n    stem = number // 10\n    leaf = number % 10\n    if stem in stem_leaf:\n        stem_leaf[stem].append(leaf)\n    else:\n        stem_leaf[stem] = [leaf]\n\n# Print stem and leaf plot\nfor stem, leaves in sorted(stem_leaf.items()):\n    print(f\"{stem} | {' '.join(map(str, sorted(leaves)))}\")\n\n2 | 0\n3 | 4 4 6 6\n4 | 5\n5 | 2 3 6 7 9\n6 | 0 1 2 3 4 5 6 7 8 9\n7 | 3 4 4 5 6 8 9\n8 | 0 1 5\n9 | 0 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html",
    "href": "ParameterEstimation.html",
    "title": "2  Parameter Estimation",
    "section": "",
    "text": "2.1 Point Estimation: Maximum Likelihood Estimators\n(Please refer the book titled “Introduction to Probability and Statistics for Engineers and Scientists” by Sheldon M Ross for more details)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "ParameterEstimation.html#interval-estimates",
    "href": "ParameterEstimation.html#interval-estimates",
    "title": "2  Parameter Estimation",
    "section": "2.2 Interval Estimates",
    "text": "2.2 Interval Estimates\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a known distribution with an unknown mean \\(\\mu\\).\nIt is established that the sample mean \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) serves as the maximum likelihood estimator for \\(\\mu\\).\nHowever, the sample mean \\(\\bar{X}\\) is not expected to be exactly equal to \\(\\mu\\), but rather close to it.\nTherefore, instead of providing a single point estimate, it is often more useful to specify an interval within which we are confident that \\(\\mu\\) lies.\nTo determine such an interval estimator, we utilize the probability distribution of the point estimator.\n\n\n2.2.1 Confidence Intervals for the Mean of a normal population with known Variance\n\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\) drawn from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nThe point estimator \\(\\bar{X}\\) is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\)/n.\nTherefore, \\(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\) follows a standard normal distribution.\n\n\n\n\n\n\n\nWhat to do\n\n\n\nConsider that I want to find an interval around \\(\\bar{X}\\) such that the actual population mean \\(\\mu\\) falls within the interval, say 95 % of the times.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor finding such an interval, I can use the Z-table. From the Z-table I can find:\n\n\\[\nP\\left( -1.96 &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; 1.96 \\right) = 0.9750 - 0.0250 = 0.95\n\\]\n\nRewriting the above equation:\n\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} -\\mu &lt; 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( 1.96\\frac{\\sigma}{\\sqrt{n}} &gt; \\mu - \\bar{X} &gt; -1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( -1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu - \\bar{X} &lt; 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\nWe have P(Z &lt; -1.96) = 0.025, similarly P(Z &gt; 1.96) = 0.025. Usually 1.96 is represented generally as \\(z_{0.025}\\). Thus, P(Z &lt; -z\\(_{0.025}\\)) = 0.025 and P(Z &gt; z\\(_{0.025}\\)) = 0.025.\nHence, 100(1-0.05) percent confidence interval for the mean of a normal population with known variance is:\n\n\\[\nP\\left( \\bar{X} - z_{0.025}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right) = 0.95\n\\]\n\\[\nP\\left( \\bar{X} - z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + z_{0.05/2}\\frac{\\sigma}{\\sqrt{n}}\\right) = (1 - 0.05)\n\\]\n\n\n\nFor a confidence level of \\(100(1-\\alpha)\\) percent, the corresponding critical value from the standard normal distribution is \\(z_{\\alpha/2}\\).\nThe \\(100(1-\\alpha)\\) percent confidence interval for \\(\\mu\\) is given by:\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.1}\\]\n\nThe interval as given in Equation 2.1 is called a two-sided confidence interval.\nAlso the term \\(z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\) is called the margin of error.\n\n\n\n\n\n\n\nDerivation of two-sided confidence interval\n\n\n\n\nTo find 100(1-\\(\\alpha\\)) percent confidence interval of mean (\\(\\mu\\)), we have;\n\n\\[\nP\\left( -z_{\\alpha/2} &lt; \\frac{\\bar{X} -\\mu}{\\sigma/\\sqrt{n}} &lt; z{_\\alpha/2} \\right) = 1 - \\alpha\n\\]\n\nDoing the same manipulations we did earlier for obtaining the 95percent confidence interval we can obtain:\n\n\\[\nP\\left( \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nThe above equation give us the required confidence interval, as given in Equation 2.1.\n\n\n\n\n\n\n\n\n\nWhat if !?\n\n\n\nWhat if we are interested in one sided confidence intervals !!?\n\n\n\n\n\n\n\n\nOne-sided Upper Confidence Iterval\n\n\n\n\nTo determine such an interval, for a standard normal random variable Z, we have;\n\n\\[\nP\\left( Z &lt; 1.645 \\right) = 0.95\n\\]\n\nThus, \\[\nP\\left( \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} &lt; 1.645 \\right) = 0.95\n\\]\n\n\\[\nP\\left( \\mu -\\bar{X} &gt; - 1.645\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}  \\right) = 0.95\n\\]\n\nThus a 95 percent one-sided upper confidence interval for \\(\\mu\\) is\n\n\\[\n\\mu \\in \\left( \\bar{X} - 1.645\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\nor in other words; 100(1-0.05) percent one-sided upper confidence interval for \\(\\mu\\) is\n\\[\n\\mu \\in \\left( \\bar{X} - z_{0.05}\\frac{\\sigma}{\\sqrt{n}}, \\infty   \\right)\n\\]\n\n\n\n\n\n\n\n\nOneside interval!\n\n\n\nCan you think of another one sided confidence interval?\n\n\n\n\n\n\n\n\nOne-sided lower confidence interval\n\n\n\n\nWe have \\[\nP\\left( Z &gt; - 1.645 \\right) = 0.95\n\\]\nProceed just like in the previous case and you will find a 100(1-0.05) percent one-sided lower confidence interval for \\(\\mu\\) as;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{0.05}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\]\n\n\n\nIn general, 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval for \\(\\mu\\) is given in Equation 2.2.\n\n\\[\n\\mu \\in \\left( \\bar{X} - z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}, \\infty \\right)\n\\tag{2.2}\\]\n\nAlso, 100(1-\\(\\alpha\\))percent one-sided lower confidence interval for \\(\\mu\\) is given in Equation 2.3. \\[\n\\mu \\in \\left( -\\infty, \\bar{X} + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\tag{2.3}\\]\nThe python code below creates a sample and find 95% confidence interval for the mean if the population standard deviation is assumed to be 10. Other values are specified in the code.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 50  # true mean\nsigma = 10  # known standard deviation\nn = 30  # sample size\nalpha = 0.05  # significance level\n\n# Generate a sample\nnp.random.seed(0)\nsample = np.random.normal(mu, sigma, n)\nsample_mean = np.mean(sample)\n\n# Calculate the confidence interval\nz = 1.96  # z-value for 95% confidence\nmargin_of_error = z * (sigma / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\n# Plot the sample and confidence interval\nplt.figure(figsize=(8, 4))\nplt.hist(sample, bins=10, alpha=0.7, color='blue', edgecolor='black')\nplt.axvline(sample_mean, color='red', linestyle='dashed', linewidth=2, label='Sample Mean')\nplt.axvline(confidence_interval[0], color='green', linestyle='dashed', linewidth=2, label='95% CI Lower Bound')\nplt.axvline(confidence_interval[1], color='green', linestyle='dashed', linewidth=2, label='95% CI Upper Bound')\nplt.title('Sample Distribution with 95% Confidence Interval')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"95% Confidence Interval: {confidence_interval}\")\n\n\n\n\n\n\n\n\nSample Mean: 54.42856447263174\n95% Confidence Interval: (50.85011043026466, 58.007018514998826)\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose that when a signal having value \\(\\mu\\) is transmitted from location A the value received at location B is normally distributed with mean \\(\\mu\\) and variance 4. That is, if \\(\\mu\\) is sent, then the value received is \\(\\mu\\) + N where N, representing noise, is normal with mean 0 and variance 4. To reduce error, suppose the same value is sent 9 times. If the successive values received are 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5;\n(a). construct a 95 percent two-sided confidence interval for \\(\\mu\\).\n(b). construct 95 percent one-sided upper and lower confidence intervals for \\(\\mu\\).\n\n\n\n\n\n\n\n\nProblem\n\n\n\nSuppose a quality control manager at a factory wants to ensure that the average weight of a product is at least 500 grams. They take a random sample of 30 products and find the sample mean weight to be 495 grams with a standard deviation of 10 grams. Help the manager to estimate the minimum average weight of the products with 95% confidence.\n\n\n\n\n2.2.2 Confidence Intervals for the Mean of a normal population with unknown Variance\n\nIf you recollect the discussion we had about the sample mean from a normal population with unknown variance we saw that variable t\\(_{n-1}\\) given by:\n\n\\[\nt_{n-1} = \\sqrt{n}\\frac{\\bar{X} - \\mu}{S}\n\\]\nhas a t-distribution with n-1 degrees of freedom.\n\nBecause of the symmetry of the t-distribution we can write for any \\(\\alpha\\) \\(\\in\\) (0, 1/2);\n\n\\[\nP\\left( -t_{\\alpha/2, n-1} &lt; \\sqrt{n}\\frac{\\bar{X} - \\mu}{S} &lt;  t_{\\alpha/2, n-1} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( -\\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &lt;  - \\mu &lt; -\\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} + t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} &gt;   \\mu &gt; \\bar{X} - t_{\\alpha/2, n-1} \\frac{\\sqrt{n}}{S} \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}}  \\right) = 1 - \\alpha\n\\]\n\nIf the sample mean is \\(\\bar{X}\\) and sample standard deviation S, then we can say that with 100(1-\\(\\alpha\\)) percent confidence that\n\n\\[\n\\mu \\in \\left(\\bar{X} - t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}},  \\bar{X} + t_{\\alpha/2, n-1} \\frac{S}{\\sqrt{n}} \\right)\n\\]\n\nIn this case 100(1-\\(\\alpha\\)) percent one-sided upper confidence interval can be obtained from the fact that:\n\n\\[\nP\\left( \\sqrt{n}\\frac{(\\bar{X} - \\mu)}{S} &lt; t_{\\alpha, n-1}\\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\mu &gt; \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right) = 1 - \\alpha  \n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided upper confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( \\bar{X} - \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}, \\infty \\right)\n\\]\n\nThus 100(1 − \\(\\alpha\\)) percent one-sided lower confidence interval for the mean in this case is given by;\n\n\\[\n\\mu \\in \\left( -\\infty, \\bar{X} + \\frac{S}{\\sqrt{n}} t_{\\alpha, n-1}  \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nLet us again consider the previous problem but let us now suppose that when the value \\(\\mu\\) is transmitted at location A then the value received at location B is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) but with \\(\\sigma^2\\) being unknown. If 9 successive values are, 5, 8.5, 12, 15, 7, 9, 7.5, 6.5, and 10.5, compute a 95 percent confidence interval for \\(\\mu\\).\n\n\n\n\n2.2.3 Confidence Intervals for the Variance of a Normal Distribution\n\nIf we are sampling from a normal distribution with unknown mean and unknown variance then;\n\n\\[\n(n-1) \\frac{S^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\]\nfollows a chi-squared distribution.\n\nWe have\n\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\chi^2_{1-\\alpha/2, n-1} \\leq (n-1)\\frac{S^2}{\\sigma^2} \\leq \\chi^2_{\\alpha/2, n-1}  \\right) = 1 - \\alpha\n\\]\n\\[\nP\\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}} \\leq \\sigma^2 \\leq \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}}  \\right) = 1 - \\alpha\n\\]\n\nHence, 100(1-\\(\\alpha\\)) percent two-sided confidence interval for the variance in this case;\n\n\\[\n\\sigma^2 \\in \\left( \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\n\\]\n\nThe 100(1-\\(\\alpha\\)) percent one-sided upper and lower confidence intervals in this case will be respectively;\n\n\\[\n\\left(\\frac{(n-1)S^2}{\\chi^2_{\\alpha, n-1}}, \\infty \\right)\n\\]\nand\n\\[\n\\left( 0, \\frac{(n-1)S^2}{\\chi^2_{1-\\alpha, n-1}} \\right)\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nA standardized procedure is expected to produce washers with very small deviation in their thicknesses. Suppose that 10 such washers were chosen and measured. If the thicknesses of these washers were, in inches; .123, .133, .124, .125, .126, .128, .120, .124, .130, and .126. What is a 90 percent confidence interval for the standard deviation of the thickness of a washer produced by this procedure?\n\n\nAll problems and most part of text are taken from Ross (2009) .\n\n\n\n\nRoss, Sheldon. 2009. “Probability and Statistics for Engineers and Scientists.” Elsevier, New Delhi 16: 32–33.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Parameter Estimation</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html",
    "href": "HypothesisTesting.html",
    "title": "3  Hypothesis Testing",
    "section": "",
    "text": "Introduction\n\\[\nH_{0}: \\theta = 1\n\\]\n\\[\nH_{0}: \\theta &gt; 1\n\\]\n\\[\nH_{0}: \\theta \\leq 1\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html#with-known-variance-z-test",
    "href": "HypothesisTesting.html#with-known-variance-z-test",
    "title": "3  Hypothesis Testing",
    "section": "4.1 With known Variance (Z-test)",
    "text": "4.1 With known Variance (Z-test)\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a known variance \\(\\sigma^2\\).\nWe are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nSince \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\) is a natural point estimator of \\(\\mu\\), it is reasonable to accept \\(H_0\\) if \\(\\bar{X}\\) is not too far from \\(\\mu_0\\).\nThus, the critical region of the test would be of the form:\n\n\\[\nC = \\{X_1, \\ldots, X_n : |\\bar{X} - \\mu_0| &gt; c\\}\n\\]\nfor some suitably chosen value \\(c\\).\n\nTo ensure that the test has a significance level \\(\\alpha\\), we must determine the critical value \\(c\\) in the above equation such that the type I error is equal to \\(\\alpha\\). This means \\(c\\) must satisfy:\n\n\\[\nP_{\\mu_0} \\{|\\bar{X} - \\mu_0| &gt; c\\} = \\alpha\n\\]\nwhere \\(P_{\\mu_0}\\) denotes that the probability is computed under the assumption that population mean, \\(\\mu = \\mu_0\\).\n\nWhen \\(\\mu = \\mu_0\\), \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\frac{\\sigma^2}{n}\\). Therefore, the standardized variable \\(Z\\) defined by:\n\n\\[\nZ = \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}}\n\\]\nwill have a standard normal distribution.\n\nThe probability of a type I error is given by:\n\n\\[\nP \\left( |\\bar{X} - \\mu_0| &gt; c \\right) = \\alpha\n\\]\n\nEquivalently, this can be written as:\n\n\\[\n2P \\left( Z &gt; \\frac{c \\sqrt{n}}{\\sigma} \\right) = \\alpha\n\\]\n\nWhere \\(Z\\) is a standard normal random variable. We know that:\n\n\\[\nP \\left( Z &gt; z_{\\alpha/2} \\right) = \\frac{\\alpha}{2}\n\\]\n\nTherefore, we have:\n\n\\[\n\\frac{c \\sqrt{n}}{\\sigma} = z_{\\alpha/2}\n\\]\n\nSolving for \\(c\\), we get:\n\n\\[\nc = \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nThus, the test at significance level \\(\\alpha\\) is to reject \\(H_0\\) if:\n\n\\[\n|\\bar{X} - \\mu_0| &gt; \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}}\n\\]\n\nAnd accept \\(H_0\\) otherwise. Equivalently, we can reject \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} &gt; z_{\\alpha/2}\n\\]\n\nAnd accept \\(H_0\\) if:\n\n\\[\n\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma} \\leq z_{\\alpha/2}\n\\]\n\n\n\n\n\n\nProblem\n\n\n\nIf a signal of value \\(\\mu\\) is sent from location A, then the value received at location B is normally distributed with mean \\(\\mu\\) and standard deviation 2. That is, the random noise added to the signal is an N(0, 4) random variable. There is reason for the people at location B to suspect that the signal value \\(\\mu\\) = 8 will be sent today. Test this hypothesis if the same signal value is independently sent five times and the average value received at location B is X = 9. 5.\n\n\n\n4.1.1 Choosing the Significance Level\n\nThe appropriate significance level \\(\\alpha\\) depends on the specific context and consequences of the hypothesis test.\nIf rejecting the null hypothesis \\(H_0\\) would lead to significant costs or consequences, a more conservative significance level (e.g., 0.05 or 0.01) should be chosen.\nIf there is a strong initial belief that \\(H_0\\) is true, strict evidence is required to reject \\(H_0\\), implying a lower significance level.\nThe test can be described as follows: For an observed value of the test statistic \\(\\sqrt{n} \\frac{|\\bar{X} - \\mu_0|}{\\sigma}\\), denoted as \\(v\\), reject \\(H_0\\) if the probability of the test statistic being as large as \\(v\\) under \\(H_0\\) is less than or equal to \\(\\alpha\\).\nThis probability is known as the p-value of the test. \\(H_0\\) is accepted if \\(\\alpha\\) is less than the p-value and rejected if \\(\\alpha\\) is greater than or equal to the p-value.\nIn practice, the significance level is sometimes not set in advance. Instead, the p-value is calculated from the data, and decisions are made based on the p-value.\nIf the p-value is much larger than any reasonable significance level, \\(H_0\\) is accepted. Conversely, if the p-value is very small, \\(H_0\\) is rejected.\n\n\n\n4.1.2 Hypothesis Testing Summary: Z- Test\n\nCaption: Summary of hypothesis testing for a sample from a \\(N(\\mu, \\sigma^2)\\) population with known \\(\\sigma^2\\).\n\n\n\n\n\n\nSample and Population\nDetails\n\n\n\n\nSample\n\\(\\{X_1, X_2, . . . , X_n\\}\\)\n\n\nPopulation\n\\(N(\\mu, \\sigma^2)\\)\n\n\nKnown Parameter\n\\(\\sigma^2\\)\n\n\nSample Mean\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\n\n\nSignificance Level\n\\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nTest Statistic (TS)\nReject if\np-Value if TS = t\n\n\n\n\n\\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(|TS| &gt; z_{\\alpha/2}\\)\n\\(2P\\{Z \\geq |t|\\}\\)\n\n\n\\(H_0: \\mu \\leq \\mu_0\\) vs \\(H_1: \\mu &gt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &gt; z_{\\alpha}\\)\n\\(P\\{Z \\geq t\\}\\)\n\n\n\\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu &lt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/\\sigma\\)\n\\(TS &lt; -z_{\\alpha}\\)\n\\(P\\{Z \\leq t\\}\\)\n\n\n\n\n\n\n\n\n\nProblem\n\n\n\nImagine you’re the quality control manager at a company that prides itself on the precision of its product weights. The company claims that the average weight of their product is exactly 100 grams. But, as a diligent manager, you decide to put this claim to the test. You randomly select a sample of 30 products and measure their weights. To your surprise, the average weight of your sample is 110 grams! Now, you need to determine if this difference is statistically significant or just a fluke. Assume the population standard deviation as 15 grams.\n\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Given data\nsample_mean = 495\npopulation_mean = 500\nstd_dev = 10\nsample_size = 30\nalpha = 0.05\n\n# Calculate the Z-score\nz_score = (sample_mean - population_mean) / (std_dev / np.sqrt(sample_size))\n\n# Calculate the p-value\np_value = stats.norm.cdf(z_score)\n\n# Determine if we reject the null hypothesis\nreject_null = p_value &lt; alpha\n\n# Output the results\nprint(f\"Z-score: {z_score}\")\nprint(f\"P-value: {p_value}\")\nprint(f\"Reject the null hypothesis: {reject_null}\")\n\nZ-score: -2.7386127875258306\nP-value: 0.00308494966027208\nReject the null hypothesis: True",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "HypothesisTesting.html#with-unknown-variance-t-test",
    "href": "HypothesisTesting.html#with-unknown-variance-t-test",
    "title": "3  Hypothesis Testing",
    "section": "4.2 With unknown Variance (T-test)",
    "text": "4.2 With unknown Variance (T-test)\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be a sample of size \\(n\\) from a normal distribution with an unknown mean \\(\\mu\\) and a unknown variance.\nSay, we are interested in testing the null hypothesis:\n\n\\[\nH_0 : \\mu = \\mu_0\n\\]\n\nAgainst the alternative hypothesis:\n\n\\[\nH_1 : \\mu \\neq \\mu_0\n\\]\n\nWhere \\(\\mu_0\\) is a specified constant.\nIn the previous case (with known variance), for a significance level (\\(\\alpha\\)) we accepted the null hypothesis if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0 }{\\sigma/\\sqrt{n}} \\right| \\leq z_{\\alpha/2}\n\\]\n\nBut in this case, \\(\\sigma\\) is unknown.\nWe know that the statistic, T, as given below has a t-distribution with n-1 degrees of freedom when \\(\\mu\\) = \\(\\mu_0\\).\n\n\\[\nT = \\frac{\\bar{X} - \\mu_0 }{S\\sqrt{n}}\n\\]\nwhere S is the sample standard deviation.\n\nHence here with \\(H_0\\): \\(\\mu\\) = \\(\\mu_0\\) and \\(H_1\\); \\(\\mu\\) \\(\\neq\\) \\(\\mu_0\\); analogous to the Z-test here in T-test we can:\n\n\n\n\n\n\n\ntwo-sided t-test\n\n\n\n\nreject the null hypothesis (\\(H_0\\)) if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} \\right| &gt; t_{\\alpha/2}\n\\]\n\naccept \\(H_0\\) if:\n\n\\[\n\\left| \\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}} \\right|  \\leq t_{\\alpha/2}\n\\]\n\n\n\n4.2.1 Hypothesis Testing Summary: T- Test\n\nCaption: Summary of hypothesis testing for a sample from a \\(N(\\mu, \\sigma^2)\\) population with unknown \\(\\sigma^2\\).\n\n\n\n\n\n\nSample and Population\nDetails\n\n\n\n\nSample\n\\(\\{X_1, X_2, . . . , X_n\\}\\)\n\n\nPopulation\n\\(N(\\mu, \\sigma^2)\\)\n\n\nSample Mean\n\\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i\\)\n\n\nSample Variance\n\\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i-\\bar{X})^2\\)\n\n\nSignificance Level\n\\(\\alpha\\)\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis\nTest Statistic (TS)\nReject if\np-Value if TS = t\n\n\n\n\n\\(H_0: \\mu = \\mu_0\\) vs \\(H_1: \\mu \\neq \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(|TS| &gt; t_{\\alpha/2, n-1}\\)\n\\(2P\\{T_{n-1} \\geq |t|\\}\\)\n\n\n\\(H_0: \\mu \\leq \\mu_0\\) vs \\(H_1: \\mu &gt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(TS &gt; t_{\\alpha, n-1}\\)\n\\(P\\{T_{n-1} \\geq t\\}\\)\n\n\n\\(H_0: \\mu \\geq \\mu_0\\) vs \\(H_1: \\mu &lt; \\mu_0\\)\n\\(\\sqrt{n}(\\bar{X} - \\mu_0)/S\\)\n\\(TS &lt; -t_{\\alpha, n-1}\\)\n\\(P\\{T_{n-1} \\leq t\\}\\)\n\n\n\n\\(\\text{\\textcolor{gray}{$T_{n−1}$ is a t-random variable with (n - 1) degrees of freedom: P($T_{n−1}$ &gt; $t_{\\alpha,n−1}$) = $\\alpha$.}}\\)\n\n\n\n\n\n\nProblem\n\n\n\nA public health official claims that the mean home water use is at most 350 gallons a day. To verify this claim, a study of 20 randomly selected homes was instigated with the result that the average daily water uses of these 20 homes were as follows:\n340 344 362 375 356 386 354 364 332 402 340 355 362 322 372 324 318 360 338 370\nDo the data contradict the official’s claim?\n\n\n\nimport numpy as np\nfrom scipy import stats\n\n# Given data\ndata = [340, 344, 362, 375, 356, 386, 354, 364, 332, 402, 340, 355, 362, 322, 372, 324, 318, 360, 338, 370]\nsample_mean = np.mean(data)\nsample_std = np.std(data, ddof=1)\nsample_size = len(data)\npopulation_mean = 350\nalpha = 0.05\n\n# Calculate the T-score\nt_score = (sample_mean - population_mean) / (sample_std / np.sqrt(sample_size))\n\n# Calculate the p-value\np_value = 2 * (1 - stats.t.cdf(np.abs(t_score), df=sample_size-1))\n\n# Determine if we reject the null hypothesis\nreject_null = p_value &lt; alpha\n\n# Output the results\nprint(f\"Sample Mean: {sample_mean}\")\nprint(f\"Sample Standard Deviation: {sample_std}\")\nprint(f\"T-score: {t_score}\")\nprint(f\"P-value: {p_value}\")\nprint(f\"Reject the null hypothesis: {reject_null}\")\n\nSample Mean: 353.8\nSample Standard Deviation: 21.847798877449275\nT-score: 0.7778411328447066\nP-value: 0.4462410900531899\nReject the null hypothesis: False",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "Anova.html",
    "href": "Anova.html",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "4.1 Introduction\nHere, we will perform an Analysis of Variance (ANOVA) to determine if there are any statistically significant differences between the means of three or more independent groups.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Anova.html#introduction",
    "href": "Anova.html#introduction",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Comparison of means of three samples\n\n\n\n\nConsider three independent samples:\n\nX: 10, 20, 30, 40\nY: 12, 21, 34, 39\nZ: 8, 11, 31, 39\n\nWe want to test if the means of these three samples are significantly different from each other.\nStep 1: Calculate the mean of each sample:\n\nMean of X: \\((10 + 20 + 30 + 40) / 4 = 25\\)\nMean of Y: \\((12 + 21 + 34 + 39) / 4 = 26.5\\)\nMean of Z: \\((8 + 11 + 31 + 39) / 4 = 22.25\\)\n\nStep 2: Calculate the overall mean (grand mean) of all the samples combined:\n\nGrand Mean: \\((10 + 20 + 30 + 40 + 12 + 21 + 34 + 39 + 8 + 11 + 31 + 39) / 12 = 24.58\\)\n\nStep 3: Analyze the variation between the sample means and the grand mean:\n\nWe can measure how much each sample mean deviates from the grand mean. To ensure all values are positive, we use the square of these differences.\n\nStep 4: Analyze the variation within each sample:\n\nWe can also measure how much each individual value in a sample deviates from its corresponding sample mean. Again, we use the square of these differences to ensure positivity.\n\nStep 5: Combine these variations to assess whether the differences between the sample means are significant relative to the variation within the samples.\nStep 6: Compare the results to a statistical distribution to determine if the observed differences are likely due to random chance or if they indicate a significant difference in the population means.\nConclusion:\n\nBased on the calculations, we compare the observed variation to a critical value. If the observed variation is less than the critical value, there is no significant difference between the sample means.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "Anova.html#one-way-analysis-of-variance",
    "href": "Anova.html#one-way-analysis-of-variance",
    "title": "4  Analysis of Variance (ANOVA)",
    "section": "4.2 One-way Analysis of Variance",
    "text": "4.2 One-way Analysis of Variance\n\nConsider m independent samples, each of size n.\n\nThe members of the ith sample are denoted as:\n\\(X_{i1}, X_{i2}, \\dots, X_{in}\\).\nEach \\(X_{ij}\\) is a normal random variable with:\n\nUnknown mean: \\(\\mu_i\\).\nUnknown variance: \\(\\sigma^2\\).\n\nMathematically:\n\\(X_{ij} \\sim N(\\mu_i, \\sigma^2)\\), where:\n\n\\(i = 1, \\dots, m\\).\n\\(j = 1, \\dots, n\\).\n\n\nHypothesis Testing:\n\nNull Hypothesis (\\(H_0\\)):\n\\(\\mu_1 = \\mu_2 = \\dots = \\mu_m\\).\n(All population means are equal.)\nAlternative Hypothesis (\\(H_1\\)):\nNot all means are equal.\n(At least two means differ.)\n\n\n\n\n\n\n\n\nInterpretation:\n\n\n\n\nImagine m different treatments.\nApplying treatment i to an item results in a normal random variable with:\n\nMean: \\(\\mu_i\\).\nVariance: \\(\\sigma^2\\).\n\nGoal: Test if all treatments have the same effect.\nMethod:\n\nApply each treatment to a different sample of n items.\nAnalyze the results to compare the means.\n\n\n\n\n\nSince there are a total of nm independent normal random variables \\(X_{ij}\\):\n\nThe sum of the squares of their standardized versions follows a chi-square distribution with nm degrees of freedom.\nMathematically:\n\\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - E[X_{ij}])^2}{\\sigma^2} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\mu_i)^2}{\\sigma^2} \\sim \\chi^2_{nm}\n\\tag{4.1}\\]\n\nTo estimate the m unknown parameters \\(\\mu_1, \\dots, \\mu_m\\):\n\nLet \\(X_{i}\\) denote the sample mean of the ith sample: \\[\n\\bar{X}_{i} = \\sum_{j=1}^{n} \\frac{X_{ij}}{n}\n\\]\nHere, \\(\\bar{X}_{i}\\) is the estimator of the population mean \\(\\mu_i\\) for \\(i = 1, \\dots, m\\).\n\nSubstituting the estimators \\(\\bar{X}_{i}\\) for \\(\\mu_i\\) in Equation Equation 4.1:\n\nThe resulting variable: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\bar{X}_{i})^2}{\\sigma^2}\n\\tag{4.2}\\] follows a chi-square distribution with nm − m degrees of freedom.\n(One degree of freedom is lost for each estimated parameter.)\n\nDefine: \\[\nSSW = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (X_{ij} - \\bar{X}_{i})^2\n\\]\n\nThe variable in Equation 4.2 becomes \\(\\frac{SSW}{\\sigma^2}\\).\n\nSince the expected value of a chi-square random variable equals its degrees of freedom:\n\nTaking the expectation of the variable in Equation Equation 4.2 gives: \\[\nE\\left[\\frac{SSW}{\\sigma^2}\\right] = nm - m\n\\]\nEquivalently: \\[\nE\\left[\\frac{SSW}{nm - m}\\right] = \\sigma^2\n\\]\n\nWe thus have our first estimator of \\(\\sigma^2\\), namely, SSW /(nm - m). The statistic SSW is called the Sum of Squares Within Samples aka ( within samples sum of squares or sum of squares within groups).\nAlso, note that this estimator was obtained without assuming anything about the truth or falsity of the null hypothesis.\nOur second estimator of \\(\\sigma^2\\) is valid only when the null hypothesis is true.\n\nAssume \\(H_0\\) is true, meaning all population means \\(\\mu_i\\) are equal, i.e., \\(\\mu_i = \\mu\\) for all \\(i\\).\nUnder this assumption:\n\nThe sample means \\(\\bar{X}_1, \\bar{X}_2, \\dots, \\bar{X}_m\\) are normally distributed with:\n\nMean: \\(\\mu\\).\nVariance: \\(\\frac{\\sigma^2}{n}\\).\n\n\n\nWe have; \\[\n\\frac{\\bar{X}_i - \\mu}{\\sqrt{\\sigma^2/n}} = \\frac{\\sqrt{n}(\\bar{X}_i - \\mu)}{\\sigma}\n\\] follows a standard normal distribution; hence, \\[\n  n \\sum_{i=1}^{m} \\frac{(\\bar{X}_i - \\mu)^2}{\\sigma^2} \\sim \\chi^2_m\n   \\tag{4.3}\\] follows a chi-square distribution with m degrees of freedom when \\(H_0\\) is true.\nWhen all population means are equal to \\(\\mu\\), the estimator of \\(\\mu\\) is the average of all nm data values, denoted as \\(\\bar{X}_{..}\\): \\[\n\\bar{X}_{..} = \\frac{\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{ij}}{nm} = \\frac{\\sum_{i=1}^{m} \\bar{X}_i}{m}\n\\]\nSubstituting \\(\\bar{X}_{..}\\) for the unknown parameter \\(\\mu\\) in Equation 4.3:\n\nWhen \\(H_0\\) is true, the resulting quantity: \\[\nn \\sum_{i=1}^{m} \\frac{(\\bar{X}_i - \\bar{X}_{..})^2}{\\sigma^2}\n\\]\n\nThe resulting quantity: \\[\nn \\sum_{i=1}^{m} \\frac{(\\bar{X}_i - \\bar{X}_{..})^2}{\\sigma^2}\n\\] will be a chi-square random variable with m - 1 degrees of freedom.\nDefine SSb as: \\[\nSSb = n \\sum_{i=1}^{m} (\\bar{X}_i - \\bar{X}_{..})^2\n\\tag{4.4}\\]\n\nIt follows that, when \\(H_0\\) is true: \\[\nSSb / \\sigma^2 \\sim \\chi^2_{m-1}\n\\]\n\nFrom the above, we derive that when \\(H_0\\) is true: \\[\nE[SSb] / \\sigma^2 = m - 1\n\\]\n\nEquivalently: \\[\nE[SSb / (m - 1)] = \\sigma^2\n\\tag{4.5}\\]\n\nTherefore, when \\(H_0\\) is true, \\(SSb / (m - 1)\\) is also an estimator of \\(\\sigma^2\\).\nThe quantity SSb Equation 4.4 is called the sum of squares between samples or between samples sum of squares or sum of squares between groups.\nNote that \\(E[SSb / (m - 1)]\\) is an estimate of \\(\\sigma^2\\) only if the hypothesis is true.\nThus we have shown that;\n\n\\(\\frac{SSW}{nm-m}\\) always estimates \\(\\sigma^2\\).\n\\(\\frac{SSb}{m-1}\\) estimates \\(\\sigma^2\\) when \\(H_0\\) is true.\n\nBecause it can be shown that \\(\\frac{SSb}{m-1}\\) will tend to exceed \\(\\sigma^2\\) when \\(H_0\\) is not true, it is reasonable to let the test statistic be given by\n\n\\[\nTS= \\frac{\\frac{SSb}{m-1} } {\\frac{SSW}{nm-m}}\n\\]\nand to reject \\(H_0\\) when \\(TS\\) is sufficiently large.\n\n4.2.1 How to do the hypothesis testing\n\nTo determine how large \\(TS\\) needs to be to justify rejecting \\(H_0\\), we use the fact that:\n\nIf \\(H_0\\) is true, then \\(SSb\\) and \\(SSW\\) are independent.\nIt follows that, when \\(H_0\\) is true, \\(TS\\) has an \\(F\\)-distribution with \\(m-1\\) numerator and \\(nm-m\\) denominator degrees of freedom.\nLet \\(F_{m-1,nm-m,\\alpha}\\) denote the \\(100(1-\\alpha)\\) percentile of this distribution — that is, \\(P\\{F_{m-1,nm-m} &gt; F_{m-1,nm-m,\\alpha}\\} = \\alpha\\).\nWe use the notation \\(F_{r,s}\\) to represent an \\(F\\)-random variable with \\(r\\) numerator and \\(s\\) denominator degrees of freedom.\n\nThe significance level \\(\\alpha\\) test of \\(H_0\\) is as follows:\n\nReject \\(H_0\\) if \\(\\frac{\\frac{SSb}{m-1} }{ \\frac{SSW}{nm-m}} &gt; F_{m-1,nm-m,\\alpha}\\).\nDo not reject \\(H_0\\) otherwise.\n\n\n\n\nSupplementary Topics\n\n\n\n\n\n\nDegrees of Freedom (dof)\n\n\n\n\nThe statement “One degree of freedom is lost for each estimated parameter” is a key concept in statistics, especially in hypothesis testing and parameter estimation. Here’s a detailed explanation:\n\n\n\n4.2.2 What Are Degrees of Freedom?\n\nDegrees of freedom (df) represent the number of independent pieces of information available to estimate a parameter or test a hypothesis.\nIn simpler terms, it’s the number of values in a calculation that are free to vary after certain constraints (like estimating parameters) are applied.\n\n\n\n\n4.2.3 Why Are Degrees of Freedom Lost?\n\nWhen you estimate parameters (e.g., population means, variances) from sample data, you use the data itself to calculate these estimates.\nEach time you estimate a parameter, you impose a constraint on the data, reducing the number of independent pieces of information available.\nExample: If you estimate the sample mean (\\(\\bar{X}\\)) from a dataset, you use the data to calculate \\(\\bar{X}\\). This means one piece of information (one degree of freedom) is “used up” in estimating \\(\\bar{X}\\), and the remaining data points are no longer fully independent.\n\n\n\n\n4.2.4 Application in the Context of the Problem\n\nIn the given problem:\n\nYou have m samples, each of size n, and you estimate the mean of each sample (\\(\\mu_i\\)) using the sample mean (\\(\\bar{X}_i\\)).\nEach time you estimate a mean (\\(\\mu_i\\)), you lose one degree of freedom because the data is used to calculate that estimate.\nSince you estimate m means (\\(\\mu_1, \\mu_2, \\dots, \\mu_m\\)), you lose m degrees of freedom in total.\n\n\n\n\n\n4.2.5 Mathematical Explanation\n\nInitially, the sum of squares: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\mu_i)^2}{\\sigma^2}\n\\] follows a chi-square distribution with nm degrees of freedom (since there are nm independent observations).\nHowever, when you replace the true means (\\(\\mu_i\\)) with their estimates (\\(\\bar{X}_i\\)), you lose m degrees of freedom (one for each estimated mean). This is because the estimates are derived from the data, reducing the independence of the observations.\nAs a result, the modified sum of squares: \\[\n\\sum_{i=1}^{m} \\sum_{j=1}^{n} \\frac{(X_{ij} - \\bar{X}_i)^2}{\\sigma^2}\n\\] follows a chi-square distribution with nm - m degrees of freedom.\n\n\n\n\n4.2.6 Why Does This Matter?\n\nDegrees of freedom affect the shape and critical values of the chi-square distribution, which is used for hypothesis testing.\nLosing degrees of freedom accounts for the fact that estimating parameters introduces uncertainty into the analysis.\nThis adjustment ensures that statistical tests (e.g., ANOVA) are accurate and reliable.\n\n\n\n\n4.2.7 Summary\n\nDegrees of freedom represent independent information in the data.\nEach estimated parameter (e.g., a mean) reduces the degrees of freedom by 1 because the data is used to calculate the estimate.\nIn the problem, estimating m means reduces the degrees of freedom from nm to nm - m, ensuring the chi-square distribution is correctly applied.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analysis of Variance (ANOVA)</span>"
    ]
  }
]